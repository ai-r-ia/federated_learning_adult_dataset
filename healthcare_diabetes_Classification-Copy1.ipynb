{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1807123e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\riash\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\riash\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Users\\riash\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.xwydx2ikjw2nmtwsfyngfuwkqu3lytcz.gfortran-win_amd64.dll\n",
      "C:\\Users\\riash\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import precision_score,recall_score, accuracy_score,confusion_matrix,f1_score\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import History\n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc3374f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: np_utils in c:\\users\\riash\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.0 in c:\\users\\riash\\anaconda3\\lib\\site-packages (from np_utils) (1.21.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b8d1d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# from keras.utils import np_utils\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Convolution2D, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling2D, MaxPooling1D\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "import csv\n",
    "from itertools import repeat\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from scipy.stats import entropy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d88681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df = pd.read_csv()\n",
    "# ### Train/Test Data seperation\n",
    "\n",
    "# file_out = pd.read_csv('Data/adult_processed.csv')\n",
    "# file_out_repaired = pd.read_csv('Data/train_data_repaired.csv')\n",
    "# cols = []\n",
    "# cols_train = []\n",
    "# for i in list(file_out.columns):\n",
    "#     if  i != 'income':\n",
    "#         cols.append(i)\n",
    "        \n",
    "# for i in list(file_out_repaired.columns):\n",
    "#     if  i != 'income':\n",
    "#         cols_train.append(i)\n",
    "\n",
    "# df_1 = pd.read_csv('Data/train_data_repaired.csv')\n",
    "# df_2 = pd.read_csv('Data/test.csv')\n",
    "\n",
    "# x = feature_set1[cols_train].copy().values\n",
    "# y = feature_set1[['income']].copy().values\n",
    "        \n",
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(x)\n",
    "# y_train = y\n",
    "\n",
    "# x2 = feature_set2[cols].copy().values\n",
    "# y2 = feature_set2[['income']].copy().values\n",
    "        \n",
    "# X_test = sc.fit_transform(x2)\n",
    "# y_test = y2\n",
    "\n",
    "\n",
    "# # X_test.shape, y_test.shape \n",
    "# if cols == cols_train:\n",
    "#     print(\"yes\")\n",
    "# # diff = np.setdiff1d(cols_train,cols)\n",
    "# # diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f6c5f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48842, 94) (48842, 1)\n",
      "(34189, 94) (34189, 1)\n",
      "(14653, 94) (14653, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(48842, 95)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "s_names = ['income']\n",
    "df = pd.read_csv('Data/adult_processed.csv')\n",
    "y_labels = df[s_names]\n",
    "x_data = df.drop(s_names, axis=1)\n",
    "\n",
    "\n",
    "print(x_data.shape, y_labels.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data,y_labels,test_size=0.3,random_state=123)\n",
    "\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b91fe167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4d737c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR TEST SPLIT\n",
    "xClients,xServer, yClients, yServer = train_test_split(X_train, y_train, test_size=0.10,random_state=523) \n",
    "\n",
    "algoName='CNN'\n",
    "x_shape, y_shape =x_data.shape[1], y_labels.shape[1]\n",
    "mean_shape = (x_shape + y_shape) // 2\n",
    "#outputClasses=len(set(y_labels))\n",
    "outputClasses=y_labels.shape[1]\n",
    "def my_metrics(y_true, y_pred):\n",
    "    accuracy=accuracy_score(y_true, y_pred)\n",
    "    precision=precision_score(y_true, y_pred,average='weighted')\n",
    "    recall=recall_score(y_true, y_pred,average='weighted')\n",
    "    f1Score=f1_score(y_true, y_pred, average='weighted') \n",
    "    print(\"Accuracy  : {}\".format(accuracy))\n",
    "    print(\"Precision : {}\".format(precision))\n",
    "    print(\"Recall : {}\".format(recall))\n",
    "    print(\"f1Score : {}\".format(f1Score))\n",
    "    \n",
    "    \n",
    "    return accuracy, precision, recall, f1Score\n",
    "\n",
    "verbose, epochs, batch_size = 0, 5, 64\n",
    "activationFun='relu'\n",
    "optimizerName='Adam'\n",
    "def createDeepModel():\n",
    "    model = Sequential()\n",
    "    \n",
    "    if(algoName=='CNN'):    \n",
    "        model.add(Conv1D(filters=10, kernel_size=3, activation=activationFun,input_shape = (x_data.shape[1], 1)))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(MaxPooling1D(pool_size=3))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv1D(filters=5, kernel_size=3, activation=activationFun))\n",
    "        model.add(Dropout(0.05))\n",
    "        model.add(MaxPooling1D(pool_size=3))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(x_shape, activation=activationFun))\n",
    "        model.add(Dense(mean_shape, activation=activationFun))\n",
    "        model.add(Dense(50, activation=activationFun))\n",
    "        model.add(Dense(outputClasses, activation='softmax'))\n",
    "        model.compile(loss='mean_squared_error', optimizer=optimizerName, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def predictTestData(yPredict, y_test): ##\n",
    "    #Converting predictions to label\n",
    "    \n",
    "    print(\"yPredict\",len(yPredict))\n",
    "    pred = list()\n",
    "    for i in range(len(yPredict)):\n",
    "        pred.append(np.argmax(yPredict[i])) ###i\n",
    "    #Converting one hot encoded test label to label\n",
    "    test = list()\n",
    "    for i in range(len(y_test)):\n",
    "        test.append(np.argmax(y_test[i]))\n",
    "    return my_metrics(test, pred)\n",
    "\n",
    "def sumOfWeights(weights):\n",
    "    return sum(map(sum, weights))\n",
    "\n",
    "def getWeights(model):\n",
    "    allLayersWeights=deepModel.get_weights()\n",
    "    return allLayersWeights\n",
    "\n",
    "# Initially train central deep model\n",
    "deepModel=createDeepModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1f21bc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_18 (Conv1D)           (None, 92, 10)            40        \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 92, 10)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 30, 10)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 30, 10)            40        \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 28, 5)             155       \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 28, 5)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 9, 5)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 9, 5)              20        \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 45)                0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 94)                4324      \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 47)                4465      \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 50)                2400      \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 11,495\n",
      "Trainable params: 11,465\n",
      "Non-trainable params: 30\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "deepModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "105a11a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfIterations= 2 #10\n",
    "numOfClients= 5  # 15, 20, 25, 30, 35, 40, 45, 50\n",
    "modelLocation='C:\\\\Users\\\\riash\\\\Desktop\\\\federated_learning_adult_dataset\\\\'+\"Models//\"+str(algoName)+\"_Sync_users_\"+str(numOfClients)+\"_\"+activationFun+\"_\"+optimizerName+\"_FL_Model.h5\"\n",
    "\n",
    "accList, precList, recallList, f1List = [], [], [], []\n",
    "\n",
    "deepModelAggWeights=[]\n",
    "\n",
    "\n",
    "firstClientFlag=True\n",
    "\n",
    "\n",
    "def updateServerModel(clientModel, clientModelWeight):\n",
    "    global firstClientFlag\n",
    "    for ind in range(len(clientModelWeight)):\n",
    "        if(firstClientFlag==True):\n",
    "            deepModelAggWeights.append(clientModelWeight[ind])            \n",
    "        else:\n",
    "            deepModelAggWeights[ind]=(deepModelAggWeights[ind]+clientModelWeight[ind])\n",
    "\n",
    "def updateClientsModels():\n",
    "    global clientsModelList\n",
    "    global deepModel\n",
    "    clientsModelList.clear()\n",
    "    for clientID in range(numOfClients):\n",
    "        m = keras.models.clone_model(deepModel)\n",
    "        m.set_weights(deepModel.get_weights())\n",
    "        clientsModelList.append(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "46aab664",
   "metadata": {},
   "outputs": [],
   "source": [
    "xServer = tf.expand_dims(xServer, axis=-1)\n",
    "X_test = tf.expand_dims(X_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f072332d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.7651 - accuracy: 0.2349\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 0s 7ms/step - loss: 0.7651 - accuracy: 0.2349\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.7651 - accuracy: 0.2349\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.7651 - accuracy: 0.2349\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.7651 - accuracy: 0.2349\n"
     ]
    }
   ],
   "source": [
    "# ----- 1. Train central model initially -----\n",
    "\n",
    "def trainInServer():\n",
    "    \n",
    "    deepModel.fit(xServer, yServer, epochs=epochs, batch_size=batch_size)\n",
    "    \n",
    "    # deepModel.fit(X_full, Y_full, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    deepModel.save(modelLocation)\n",
    "    \n",
    "trainInServer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "df89776d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of datasets owned by clients : 30770\n",
      "6896\n",
      "6466\n",
      "7995\n",
      "4915\n",
      "4498\n"
     ]
    }
   ],
   "source": [
    "# ------- 2. Separate clients data into lists ----------\n",
    "\n",
    "xClientsList=[]\n",
    "yClientsList=[]\n",
    "clientsModelList=[]\n",
    "\n",
    "\n",
    "num_dataset = len(xClients)\n",
    "lastLowerBound=0\n",
    "min_num = 4000\n",
    "max_num = 8000\n",
    "clientDataInterval=[]\n",
    "while(sum(clientDataInterval)!=len(xClients)):\n",
    "    clientDataInterval = np.random.randint(min_num, max_num+1, size=numOfClients)\n",
    "\n",
    "print(f\"Total number of datasets owned by clients : {sum(clientDataInterval)}\")\n",
    "\n",
    "#print(f\"Total number of datasets owned by clients after dublication: {sum(df_2)}\")\n",
    "\n",
    "\n",
    "# divide and assign\n",
    "\n",
    "for clientID in range(numOfClients):\n",
    "    xClientsList.append(xClients[lastLowerBound : lastLowerBound+clientDataInterval[clientID]])\n",
    "    yClientsList.append(yClients[lastLowerBound : lastLowerBound+clientDataInterval[clientID]])\n",
    "    print(len(xClientsList[clientID]))\n",
    "    \n",
    "   \n",
    "    s_name = ['income']\n",
    "    \n",
    "#     xCl= X_single.drop(s_name, axis=1)\n",
    "#     yCl = X_single[s_name]\n",
    "#     xCl= xClients.drop(s_name, axis=1)\n",
    "#     yCl = X_single[s_name]\n",
    "#     xClientsList_C.append(xCl)\n",
    "#     yClientsList_C.append(yCl)\n",
    "    \n",
    "    \n",
    "#     xCl= X_i.drop(s_name, axis=1)\n",
    "#     yCl = X_i[s_name]\n",
    "\n",
    "    model=load_model(modelLocation)\n",
    "    clientsModelList.append(model)\n",
    "    lastLowerBound+=clientDataInterval[clientID]\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "df524983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6896, 6466, 7995, 4915, 4498])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clientDataInterval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9e1ed9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20668</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39609</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15858</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41078</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36244</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16106</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7489</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29206</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14653 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       income\n",
       "20668       0\n",
       "1722        0\n",
       "39609       0\n",
       "15858       0\n",
       "41078       0\n",
       "...       ...\n",
       "36244       1\n",
       "16106       0\n",
       "7489        0\n",
       "469         1\n",
       "29206       0\n",
       "\n",
       "[14653 rows x 1 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f08d195a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clientID 0\n",
      "client size 6896\n",
      "clientID 1\n",
      "client size 6466\n",
      "clientID 2\n",
      "client size 7995\n",
      "clientID 3\n",
      "client size 4915\n",
      "clientID 4\n",
      "client size 4498\n"
     ]
    }
   ],
   "source": [
    " for clientID in range(numOfClients):\n",
    "    xClientsList[clientID] = tf.expand_dims(xClientsList[clientID], axis=-1)\n",
    "    print(\"clientID\",clientID)\n",
    "    print(\"client size\",len(xClientsList[clientID]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a0f6fe9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clientId 0\n",
      "Epoch 1/5\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7616 - accuracy: 0.2384\n",
      "Epoch 2/5\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7616 - accuracy: 0.2384\n",
      "Epoch 3/5\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.7616 - accuracy: 0.2384\n",
      "Epoch 4/5\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.7616 - accuracy: 0.2384\n",
      "Epoch 5/5\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.7616 - accuracy: 0.2384\n",
      "clientId 1\n",
      "Epoch 1/5\n",
      "102/102 [==============================] - 1s 7ms/step - loss: 0.7561 - accuracy: 0.2439\n",
      "Epoch 2/5\n",
      "102/102 [==============================] - 0s 5ms/step - loss: 0.7561 - accuracy: 0.2439\n",
      "Epoch 3/5\n",
      "102/102 [==============================] - 1s 5ms/step - loss: 0.7561 - accuracy: 0.2439\n",
      "Epoch 4/5\n",
      "102/102 [==============================] - 1s 7ms/step - loss: 0.7561 - accuracy: 0.2439\n",
      "Epoch 5/5\n",
      "102/102 [==============================] - 1s 6ms/step - loss: 0.7561 - accuracy: 0.2439\n",
      "clientId 2\n",
      "Epoch 1/5\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.7635 - accuracy: 0.2365\n",
      "Epoch 2/5\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.7635 - accuracy: 0.2365\n",
      "Epoch 3/5\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.7635 - accuracy: 0.2365\n",
      "Epoch 4/5\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.7635 - accuracy: 0.2365\n",
      "Epoch 5/5\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.7635 - accuracy: 0.2365\n",
      "clientId 3\n",
      "Epoch 1/5\n",
      "77/77 [==============================] - 1s 7ms/step - loss: 0.7465 - accuracy: 0.2535\n",
      "Epoch 2/5\n",
      "77/77 [==============================] - 1s 7ms/step - loss: 0.7465 - accuracy: 0.2535\n",
      "Epoch 3/5\n",
      "77/77 [==============================] - 1s 7ms/step - loss: 0.7465 - accuracy: 0.2535\n",
      "Epoch 4/5\n",
      "77/77 [==============================] - 0s 6ms/step - loss: 0.7465 - accuracy: 0.2535\n",
      "Epoch 5/5\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.7465 - accuracy: 0.2535\n",
      "clientId 4\n",
      "Epoch 1/5\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 0.7601 - accuracy: 0.2399\n",
      "Epoch 2/5\n",
      "71/71 [==============================] - 1s 7ms/step - loss: 0.7601 - accuracy: 0.2399\n",
      "Epoch 3/5\n",
      "71/71 [==============================] - 1s 7ms/step - loss: 0.7601 - accuracy: 0.2399\n",
      "Epoch 4/5\n",
      "71/71 [==============================] - 0s 7ms/step - loss: 0.7601 - accuracy: 0.2399\n",
      "Epoch 5/5\n",
      "71/71 [==============================] - 1s 7ms/step - loss: 0.7601 - accuracy: 0.2399\n",
      "Iteration 1\n",
      "clientID 0\n",
      "client size 6896\n",
      "Epoch 1/5\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7616 - accuracy: 0.2384\n",
      "Epoch 2/5\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.7616 - accuracy: 0.2384\n",
      "Epoch 3/5\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.7616 - accuracy: 0.2384\n",
      "Epoch 4/5\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.7616 - accuracy: 0.2384\n",
      "Epoch 5/5\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.7616 - accuracy: 0.2384\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'loc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 29\u001b[0m\n\u001b[0;32m     25\u001b[0m yPredict \u001b[38;5;241m=\u001b[39m (clientsModelList[clientID]\u001b[38;5;241m.\u001b[39mpredict(X_test)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0.50\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     27\u001b[0m indexes \u001b[38;5;241m=\u001b[39m y_test\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m---> 29\u001b[0m X_t \u001b[38;5;241m=\u001b[39m \u001b[43mX_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m[indexes,:]\n\u001b[0;32m     30\u001b[0m y_t \u001b[38;5;241m=\u001b[39m y_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreadmitted_<30\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     31\u001b[0m y_p\u001b[38;5;241m=\u001b[39m yPredict[:,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'loc'"
     ]
    }
   ],
   "source": [
    "# ------- 3. Update clients' model with intial server's deep-model ----------\n",
    "for clientID in range(0,numOfClients):\n",
    "    print(\"clientId\", clientID)\n",
    "    clientsModelList[clientID].fit(xClientsList[clientID], yClientsList[clientID], epochs=epochs, batch_size=batch_size)\n",
    "    \n",
    "start_time = time.time()\n",
    "process = psutil.Process(os.getpid())\n",
    "cols, rows=numOfClients,numOfIterations\n",
    "\n",
    "for iterationNo in range(1,numOfIterations+1):\n",
    "    print(\"Iteration\",iterationNo)\n",
    "    \n",
    "    for clientID in range(numOfClients):\n",
    "        print(\"clientID\",clientID)\n",
    "        print(\"client size\",len(xClientsList[clientID]))\n",
    "        \n",
    "        #'readmitted_0','readmitted_1' race_AfricanAmerican, race_Caucasian\n",
    "        \n",
    "        clientsModelList[clientID].compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        clientsModelList[clientID].fit(xClientsList[clientID], yClientsList[clientID], epochs=epochs, batch_size=batch_size)\n",
    "        clientWeight=clientsModelList[clientID].get_weights()\n",
    "        \n",
    "        \n",
    "        yPredict = (clientsModelList[clientID].predict(X_test)>0.50)*1\n",
    "        \n",
    "        indexes = y_test.index.tolist()\n",
    "        \n",
    "        X_t = X_test.loc[indexes,:]\n",
    "        y_t = y_test['income'].tolist()\n",
    "        y_p= yPredict[:,0].tolist()\n",
    "\n",
    "        # Find sum of all client's model\n",
    "        updateServerModel(clientsModelList[clientID], clientWeight)\n",
    "        \n",
    "        \n",
    "        firstClientFlag=False\n",
    "        \n",
    "    arr.append(d_i)\n",
    "   \n",
    "    #Average all clients model\n",
    "    for ind in range(len(deepModelAggWeights)):\n",
    "        deepModelAggWeights[ind]/=numOfClients\n",
    "\n",
    "    dw_last=deepModel.get_weights()\n",
    "\n",
    "    for ind in range(len(deepModelAggWeights)): \n",
    "        dw_last[ind]=deepModelAggWeights[ind]\n",
    "\n",
    "        \n",
    "    #Update server's model\n",
    "    deepModel.set_weights(dw_last)\n",
    "     \n",
    "    print(\"Server's model updated\")\n",
    "    print(\"Saving model . . .\")\n",
    "    deepModel.save(modelLocation)\n",
    "    # Servers model is updated, now it can be used again by the clients\n",
    "    updateClientsModels()\n",
    "    firstClientFlag=True\n",
    "    deepModelAggWeights.clear()\n",
    "    \n",
    "    # Servers model is updated, now it can be used again by the clients\n",
    "\n",
    "\n",
    "memoryTraining=process.memory_percent()\n",
    "timeTraining=time.time() - start_time\n",
    "print(\"---Memory---\",memoryTraining)\n",
    "print(\"--- %s seconds (TRAINING)---\" % (timeTraining))\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='auto')\n",
    "\n",
    "hist = deepModel.fit(xServer, yServer, epochs=epochs, \n",
    "                        validation_data = (X_test,y_test))\n",
    "                        # callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "learningAccs=hist.history['val_accuracy']\n",
    "learningLoss=hist.history['val_loss']\n",
    "\n",
    "\n",
    "# resultSaveLocation=root_path+'Results/'+algoName+'_Users_vs_TR_vs_Iterations_vs_AccLossMemTime'+'.csv'\n",
    "dfSave=pd.DataFrame(columns=['Clients', 'Iterations to converge', 'Accuracy', 'Loss', 'Memory', 'Time'])\n",
    "dfSaveIndex=0\n",
    "saveList = [numOfClients, len(learningLoss), learningAccs[len(learningAccs)-1], learningLoss[len(learningLoss)-1], memoryTraining, timeTraining]\n",
    "dfSave.loc[dfSaveIndex] = saveList\n",
    "\n",
    "yPredict = (deepModel.predict(X_test)>0.50)*1\n",
    "\n",
    "\n",
    "#acc, prec, recall, f1Score= predictTestData(yPredict,y_test)\n",
    "\n",
    "print(\"Number of users:\", numOfClients)\n",
    "deepModel.save(modelLocation)\n",
    "print(\"Epochs:\", epochs)\n",
    "print(\"BatchSize:\", batch_size)\n",
    "print(\"Activation:\", activationFun, \"Optimizer:\", optimizerName)\n",
    "\n",
    "print(\"Iterations:\", numOfIterations)\n",
    "print(\"Memory:\", memoryTraining)\n",
    "print(\"Time:\", timeTraining)\n",
    "print(dfSave)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
