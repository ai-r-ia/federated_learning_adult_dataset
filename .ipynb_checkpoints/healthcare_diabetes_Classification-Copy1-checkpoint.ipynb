{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1807123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import precision_score,recall_score, accuracy_score,confusion_matrix,f1_score\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import History\n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b8d1d88",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.compat.v2' has no attribute '__internal__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m np_utils\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dropout\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Flatten, BatchNormalization\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\__init__.py:21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\models\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras models API.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Functional\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py:26\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layout_map \u001b[38;5;28;01mas\u001b[39;00m layout_map_lib\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base_layer\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\backend.py:32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend_config\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_coordinator_utils \u001b[38;5;28;01mas\u001b[39;00m dc\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtensor_api \u001b[38;5;28;01mas\u001b[39;00m dtensor\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\backend_config.py:33\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Default image data format, one of \"channels_last\", \"channels_first\".\u001b[39;00m\n\u001b[0;32m     29\u001b[0m _IMAGE_DATA_FORMAT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannels_last\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.backend.epsilon\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mepsilon\u001b[39m():\n\u001b[0;32m     35\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the value of the fuzz factor used in numeric expressions.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    1e-07\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _EPSILON\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.compat.v2' has no attribute '__internal__'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Convolution2D, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling2D, MaxPooling1D\n",
    "from keras import backend as K\n",
    "from keras import backend\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "import csv\n",
    "from itertools import repeat\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from scipy.stats import entropy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d88681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df = pd.read_csv()\n",
    "# ### Train/Test Data seperation\n",
    "\n",
    "# file_out = pd.read_csv('Data/adult_processed.csv')\n",
    "# file_out_repaired = pd.read_csv('Data/train_data_repaired.csv')\n",
    "# cols = []\n",
    "# cols_train = []\n",
    "# for i in list(file_out.columns):\n",
    "#     if  i != 'income':\n",
    "#         cols.append(i)\n",
    "        \n",
    "# for i in list(file_out_repaired.columns):\n",
    "#     if  i != 'income':\n",
    "#         cols_train.append(i)\n",
    "\n",
    "# df_1 = pd.read_csv('Data/train_data_repaired.csv')\n",
    "# df_2 = pd.read_csv('Data/test.csv')\n",
    "\n",
    "# x = feature_set1[cols_train].copy().values\n",
    "# y = feature_set1[['income']].copy().values\n",
    "        \n",
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(x)\n",
    "# y_train = y\n",
    "\n",
    "# x2 = feature_set2[cols].copy().values\n",
    "# y2 = feature_set2[['income']].copy().values\n",
    "        \n",
    "# X_test = sc.fit_transform(x2)\n",
    "# y_test = y2\n",
    "\n",
    "\n",
    "# # X_test.shape, y_test.shape \n",
    "# if cols == cols_train:\n",
    "#     print(\"yes\")\n",
    "# # diff = np.setdiff1d(cols_train,cols)\n",
    "# # diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6c5f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# s_names = ['readmitted_<30','readmitted_>30']\n",
    "df = pd.read_csv('Data/adult_processed.csv')\n",
    "x_data = df.drop('income', axis=1)\n",
    "y_labels = df['income']\n",
    "\n",
    "print(x_data.shape, y_labels.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data,y_labels,test_size=0.3,random_state=123)\n",
    "\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d737c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR TEST SPLIT\n",
    "xClients,xServer, yClients, yServer = train_test_split(X_train, y_train, test_size=0.10,random_state=523) \n",
    "\n",
    "algoName='CNN'\n",
    "x_shape, y_shape =x_data.shape[1], y_labels.shape[1]\n",
    "mean_shape = (x_shape + y_shape) // 2\n",
    "#outputClasses=len(set(y_labels))\n",
    "outputClasses=y_labels.shape[1]\n",
    "def my_metrics(y_true, y_pred):\n",
    "    accuracy=accuracy_score(y_true, y_pred)\n",
    "    precision=precision_score(y_true, y_pred,average='weighted')\n",
    "    recall=recall_score(y_true, y_pred,average='weighted')\n",
    "    f1Score=f1_score(y_true, y_pred, average='weighted') \n",
    "    print(\"Accuracy  : {}\".format(accuracy))\n",
    "    print(\"Precision : {}\".format(precision))\n",
    "    print(\"Recall : {}\".format(recall))\n",
    "    print(\"f1Score : {}\".format(f1Score))\n",
    "    \n",
    "    \n",
    "    return accuracy, precision, recall, f1Score\n",
    "\n",
    "verbose, epochs, batch_size = 0, 10, 64\n",
    "activationFun='relu'\n",
    "optimizerName='Adam'\n",
    "def createDeepModel():\n",
    "    model = Sequential()\n",
    "    \n",
    "    if(algoName=='CNN'):    \n",
    "        model.add(Conv1D(filters=10, kernel_size=3, activation=activationFun,input_shape = (x_data.shape[1], 1)))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(MaxPooling1D(pool_size=3))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv1D(filters=5, kernel_size=3, activation=activationFun))\n",
    "        model.add(Dropout(0.05))\n",
    "        model.add(MaxPooling1D(pool_size=3))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(x_shape, activation=activationFun))\n",
    "        model.add(Dense(mean_shape, activation=activationFun))\n",
    "        model.add(Dense(50, activation=activationFun))\n",
    "        model.add(Dense(outputClasses, activation='softmax'))\n",
    "        model.compile(loss='mean_squared_error', optimizer=optimizerName, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def predictTestData(yPredict, y_test): ##\n",
    "    #Converting predictions to label\n",
    "    \n",
    "    print(\"yPredict\",len(yPredict))\n",
    "    pred = list()\n",
    "    for i in range(len(yPredict)):\n",
    "        pred.append(np.argmax(yPredict[i])) ###i\n",
    "    #Converting one hot encoded test label to label\n",
    "    test = list()\n",
    "    for i in range(len(y_test)):\n",
    "        test.append(np.argmax(y_test[i]))\n",
    "    return my_metrics(test, pred)\n",
    "\n",
    "def sumOfWeights(weights):\n",
    "    return sum(map(sum, weights))\n",
    "\n",
    "def getWeights(model):\n",
    "    allLayersWeights=deepModel.get_weights()\n",
    "    return allLayersWeights\n",
    "\n",
    "# Initially train central deep model\n",
    "deepModel=createDeepModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f21bc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105a11a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfIterations= 10\n",
    "numOfClients= 5  # 15, 20, 25, 30, 35, 40, 45, 50\n",
    "modelLocation='C:\\\\Users\\\\jpath\\\\Healthcare\\\\'+\"Models//\"+str(algoName)+\"_Sync_users_\"+str(numOfClients)+\"_\"+activationFun+\"_\"+optimizerName+\"_FL_Model.h5\"\n",
    "\n",
    "accList, precList, recallList, f1List = [], [], [], []\n",
    "\n",
    "deepModelAggWeights=[]\n",
    "\n",
    "\n",
    "firstClientFlag=True\n",
    "\n",
    "\n",
    "def updateServerModel(clientModel, clientModelWeight):\n",
    "    global firstClientFlag\n",
    "    for ind in range(len(clientModelWeight)):\n",
    "        if(firstClientFlag==True):\n",
    "            deepModelAggWeights.append(clientModelWeight[ind])            \n",
    "        else:\n",
    "            deepModelAggWeights[ind]=(deepModelAggWeights[ind]+clientModelWeight[ind])\n",
    "\n",
    "def updateClientsModels():\n",
    "    global clientsModelList\n",
    "    global deepModel\n",
    "    clientsModelList.clear()\n",
    "    for clientID in range(numOfClients):\n",
    "        m = keras.models.clone_model(deepModel)\n",
    "        m.set_weights(deepModel.get_weights())\n",
    "        clientsModelList.append(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f072332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 1. Train central model initially -----\n",
    "def trainInServer():\n",
    "    deepModel.fit(xServer, yServer, epochs=epochs, batch_size=batch_size)\n",
    "    \n",
    "    # deepModel.fit(X_full, Y_full, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    deepModel.save(modelLocation)\n",
    "    \n",
    "trainInServer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df89776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- 2. Separate clients data into lists ----------\n",
    "\n",
    "xClientsList=[]\n",
    "yClientsList=[]\n",
    "clientsModelList=[]\n",
    "\n",
    "\n",
    "num_dataset = len(xClients)\n",
    "lastLowerBound=0\n",
    "min_num = 4000\n",
    "max_num = 8000\n",
    "clientDataInterval=[]\n",
    "while(sum(clientDataInterval)!=len(xClients)):\n",
    "    clientDataInterval = np.random.randint(min_num, max_num+1, size=numOfClients)\n",
    "\n",
    "print(f\"Total number of datasets owned by clients : {sum(clientDataInterval)}\")\n",
    "\n",
    "#print(f\"Total number of datasets owned by clients after dublication: {sum(df_2)}\")\n",
    "\n",
    "\n",
    "# divide and assign\n",
    "\n",
    "for clientID in range(numOfClients):\n",
    "    xClientsList.append(xClients[lastLowerBound : lastLowerBound+clientDataInterval[clientID]])\n",
    "    yClientsList.append(yClients[lastLowerBound : lastLowerBound+clientDataInterval[clientID]])\n",
    "    print(len(xClientsList[clientID]))\n",
    "    \n",
    "   \n",
    "    s_name = ['readmitted_<30','readmitted_>30']\n",
    "    \n",
    "    xCl= X_single.drop(s_name, axis=1)\n",
    "    yCl = X_single[s_name]\n",
    "    xClientsList_C.append(xCl)\n",
    "    yClientsList_C.append(yCl)\n",
    "    \n",
    "    \n",
    "    xCl= X_i.drop(s_name, axis=1)\n",
    "    yCl = X_i[s_name]\n",
    "\n",
    "    model=load_model(modelLocation)\n",
    "    clientsModelList.append(model)\n",
    "    lastLowerBound+=clientDataInterval[clientID]\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df524983",
   "metadata": {},
   "outputs": [],
   "source": [
    "clientDataInterval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1ed9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f6fe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- 3. Update clients' model with intial server's deep-model ----------\n",
    "for clientID in range(0,numOfClients):\n",
    "    print(\"clientId\", clientID)\n",
    "    clientsModelList[clientID].fit(xClientsList[clientID], yClientsList[clientID], epochs=epochs, batch_size=batch_size)\n",
    "    \n",
    "start_time = time.time()\n",
    "process = psutil.Process(os.getpid())\n",
    "cols, rows=numOfClients,numOfIterations\n",
    "\n",
    "for iterationNo in range(1,numOfIterations+1):\n",
    "    print(\"Iteration\",iterationNo)\n",
    "    \n",
    "    for clientID in range(numOfClients):\n",
    "        print(\"clientID\",clientID)\n",
    "        print(\"client size\",len(xClientsList[clientID]))\n",
    "        \n",
    "        #'readmitted_0','readmitted_1' race_AfricanAmerican, race_Caucasian\n",
    "        clientsModelList[clientID].compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "        clientsModelList[clientID].fit(xClientsList[clientID], yClientsList[clientID], epochs=epochs, batch_size=batch_size)\n",
    "        clientWeight=clientsModelList[clientID].get_weights()\n",
    "        \n",
    "        \n",
    "        yPredict = (clientsModelList[clientID].predict(X_test)>0.50)*1\n",
    "        \n",
    "        indexes = y_test.index.tolist()\n",
    "        \n",
    "        X_t = X_test.loc[indexes,:]\n",
    "        y_t = y_test['readmitted_<30'].tolist()\n",
    "        y_p= yPredict[:,0].tolist()\n",
    "\n",
    "        # Find sum of all client's model\n",
    "        updateServerModel(clientsModelList[clientID], clientWeight)\n",
    "        \n",
    "        \n",
    "        firstClientFlag=False\n",
    "        \n",
    "    arr.append(d_i)\n",
    "   \n",
    "    #Average all clients model\n",
    "    for ind in range(len(deepModelAggWeights)):\n",
    "        deepModelAggWeights[ind]/=numOfClients\n",
    "\n",
    "    dw_last=deepModel.get_weights()\n",
    "\n",
    "    for ind in range(len(deepModelAggWeights)): \n",
    "        dw_last[ind]=deepModelAggWeights[ind]\n",
    "\n",
    "        \n",
    "    #Update server's model\n",
    "    deepModel.set_weights(dw_last)\n",
    "     \n",
    "    print(\"Server's model updated\")\n",
    "    print(\"Saving model . . .\")\n",
    "    deepModel.save(modelLocation)\n",
    "    # Servers model is updated, now it can be used again by the clients\n",
    "    updateClientsModels()\n",
    "    firstClientFlag=True\n",
    "    deepModelAggWeights.clear()\n",
    "    \n",
    "    # Servers model is updated, now it can be used again by the clients\n",
    "\n",
    "\n",
    "memoryTraining=process.memory_percent()\n",
    "timeTraining=time.time() - start_time\n",
    "print(\"---Memory---\",memoryTraining)\n",
    "print(\"--- %s seconds (TRAINING)---\" % (timeTraining))\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='auto')\n",
    "\n",
    "hist = deepModel.fit(xServer, yServer, epochs=epochs, \n",
    "                        validation_data = (X_test,y_test))\n",
    "                        # callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "learningAccs=hist.history['val_accuracy']\n",
    "learningLoss=hist.history['val_loss']\n",
    "\n",
    "\n",
    "# resultSaveLocation=root_path+'Results/'+algoName+'_Users_vs_TR_vs_Iterations_vs_AccLossMemTime'+'.csv'\n",
    "dfSave=pd.DataFrame(columns=['Clients', 'Iterations to converge', 'Accuracy', 'Loss', 'Memory', 'Time'])\n",
    "dfSaveIndex=0\n",
    "saveList = [numOfClients, len(learningLoss), learningAccs[len(learningAccs)-1], learningLoss[len(learningLoss)-1], memoryTraining, timeTraining]\n",
    "dfSave.loc[dfSaveIndex] = saveList\n",
    "\n",
    "yPredict = (deepModel.predict(X_test)>0.50)*1\n",
    "\n",
    "\n",
    "#acc, prec, recall, f1Score= predictTestData(yPredict,y_test)\n",
    "\n",
    "print(\"Number of users:\", numOfClients)\n",
    "deepModel.save(modelLocation)\n",
    "print(\"Epochs:\", epochs)\n",
    "print(\"BatchSize:\", batch_size)\n",
    "print(\"Activation:\", activationFun, \"Optimizer:\", optimizerName)\n",
    "\n",
    "print(\"Iterations:\", numOfIterations)\n",
    "print(\"Memory:\", memoryTraining)\n",
    "print(\"Time:\", timeTraining)\n",
    "print(dfSave)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
