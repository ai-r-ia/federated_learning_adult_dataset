{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cb54a96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T12:48:49.512456Z",
     "start_time": "2021-07-15T12:48:46.946937Z"
    }
   },
   "source": [
    "Repairing the dataset at client level.\n",
    "\n",
    "Note: This has not been implemented in this notebook. To implement please make changes to the architecture and distribute the dataframe amongst clients before converting it into batched datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09b7dd46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T23:57:57.590963Z",
     "start_time": "2021-07-15T23:57:57.572204Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\riash\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\riash\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Users\\riash\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.xwydx2ikjw2nmtwsfyngfuwkqu3lytcz.gfortran-win_amd64.dll\n",
      "C:\\Users\\riash\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import precision_score,recall_score, accuracy_score,confusion_matrix,f1_score\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import History\n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00b9bf6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T23:59:52.574848Z",
     "start_time": "2021-07-15T23:59:52.208912Z"
    }
   },
   "outputs": [],
   "source": [
    "### Train/Test Data seperation\n",
    "\n",
    "file_out = pd.read_csv('Data/adult_processed.csv')\n",
    "cols = []\n",
    "\n",
    "for i in list(file_out.columns):\n",
    "    if  i != 'income':\n",
    "        cols.append(i)\n",
    "\n",
    "feature_set1 = pd.read_csv('Data/train.csv')\n",
    "feature_set2 = pd.read_csv('Data/test.csv')\n",
    "\n",
    "x = feature_set1[cols].copy().values\n",
    "y = feature_set1[['income']].copy().values\n",
    "        \n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(x)\n",
    "y_train = y\n",
    "\n",
    "x2 = feature_set2[cols].copy().values\n",
    "y2 = feature_set2[['income']].copy().values\n",
    "        \n",
    "X_test = sc.fit_transform(x2)\n",
    "y_test = y2\n",
    "\n",
    "\n",
    "# X_test.shape, y_test.shape \n",
    "# if cols == cols_train:\n",
    "#     print(\"yes\")\n",
    "# diff = np.setdiff1d(cols_train,cols)\n",
    "# diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76379dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender split\n",
    "\n",
    "dict_users = {i: np.array([]) for i in range(10)}\n",
    "data_out = []\n",
    "\n",
    "def create_hetero_clients( image_list, label_list, start_client = 0, num_clients=10, initial='clients'):\n",
    "    \n",
    "    selected_inds = []\n",
    "\n",
    "    max_y = np.argmax(label_list, axis=-1)\n",
    "    sorted_zip = sorted(zip(max_y, label_list, image_list), key=lambda x: x[0])\n",
    "    data = [(x,y) for _,y,x in sorted_zip]\n",
    "    \n",
    "    data_out = data\n",
    "    \n",
    "    num_shards, num_imgs = int(len(image_list)/30), 30\n",
    "    idx_shard = [i for i in range(num_shards)]\n",
    "\n",
    "    min_shard = 1\n",
    "    max_shard = 60  #953/15 = 63.53\n",
    "    \n",
    "    random_shard_size = np.random.randint(min_shard, max_shard+1,\n",
    "                                          size=(num_clients-start_client))\n",
    "    random_shard_size = np.around(random_shard_size /\n",
    "                                  sum(random_shard_size) * num_shards)\n",
    "    random_shard_size = random_shard_size.astype(int)\n",
    "\n",
    "\n",
    "    if sum(random_shard_size) > num_shards:\n",
    "        \n",
    "        for i in range(start_client, num_clients):\n",
    "            # First assign each client 1 shard to ensure every client has\n",
    "            # atleast one shard of data\n",
    "            rand_set = set(np.random.choice(idx_shard, 1, replace=False))\n",
    "            idx_shard = list(set(idx_shard) - rand_set)\n",
    "            for rand in rand_set:\n",
    "                ind1 = rand*num_imgs\n",
    "                ind2 = (rand+1)*num_imgs\n",
    "                if(len(dict_users[i]) == 0):\n",
    "                    dict_users[i] = data[ind1: ind2]\n",
    "                else:\n",
    "                    dict_users[i] = np.concatenate(\n",
    "                    (dict_users[i],data[ind1: ind2]),\n",
    "                    axis=0)\n",
    "                selected_inds.extend([[ind1, ind2]])\n",
    "\n",
    "        random_shard_size = random_shard_size-1\n",
    "\n",
    "        # Next, randomly assign the remaining shards\n",
    "        for i in range(start_client, num_clients):\n",
    "            if len(idx_shard) == 0:\n",
    "                continue\n",
    "#             print(random_shard_size)\n",
    "            shard_size = random_shard_size[i-start_client]\n",
    "            if shard_size > len(idx_shard):\n",
    "                shard_size = len(idx_shard)\n",
    "            rand_set = set(np.random.choice(idx_shard, shard_size,\n",
    "                                            replace=False))\n",
    "            idx_shard = list(set(idx_shard) - rand_set)\n",
    "            for rand in rand_set:\n",
    "                ind1 = rand*num_imgs\n",
    "                ind2 = (rand+1)*num_imgs\n",
    "                if(len(dict_users[i]) == 0):\n",
    "                    dict_users[i] = data[ind1: ind2]\n",
    "                else:\n",
    "                    dict_users[i] = np.concatenate(\n",
    "                    (dict_users[i],data[ind1: ind2]),\n",
    "                    axis=0)\n",
    "                selected_inds.extend([[ind1, ind2]])\n",
    "\n",
    "    else:\n",
    "\n",
    "        for i in range(start_client, num_clients):\n",
    "#             print(random_shard_size)\n",
    "            shard_size = random_shard_size[i-start_client]\n",
    "#             shard_size = random_shard_size[int(i/len(random_shard_size)) - 1]\n",
    "            rand_set = set(np.random.choice(idx_shard, shard_size,\n",
    "                                            replace=False))\n",
    "            idx_shard = list(set(idx_shard) - rand_set)\n",
    "            for rand in rand_set:\n",
    "                ind1 = rand*num_imgs\n",
    "                ind2 = (rand+1)*num_imgs\n",
    "                if(len(dict_users[i]) == 0):\n",
    "                    dict_users[i] = data[ind1: ind2]\n",
    "                else:\n",
    "                    dict_users[i] = np.concatenate(\n",
    "                    (dict_users[i],data[ind1: ind2]),\n",
    "                    axis=0)\n",
    "                selected_inds.extend([[ind1, ind2]])\n",
    "\n",
    "        if len(idx_shard) > 0:\n",
    "            # Add the leftover shards to the client with minimum images:\n",
    "            shard_size = len(idx_shard)\n",
    "            # Add the remaining shard to the client with lowest data\n",
    "            k = min(dict_users, key=lambda x: len(dict_users.get(x)))\n",
    "            rand_set = set(np.random.choice(idx_shard, shard_size,\n",
    "                                            replace=False))\n",
    "            idx_shard = list(set(idx_shard) - rand_set)\n",
    "            for rand in rand_set:\n",
    "                ind1 = rand*num_imgs\n",
    "                ind2 = (rand+1)*num_imgs\n",
    "                if(len(dict_users[k]) == 0):\n",
    "                    dict_users[k] = data[ind1: ind2]\n",
    "                else:\n",
    "                    dict_users[k] = np.concatenate(\n",
    "                    (dict_users[k],data[ind1: ind2]),\n",
    "                    axis=0)\n",
    "                selected_inds.extend([[ind1, ind2]])\n",
    "                \n",
    "                \n",
    "    return dict_users, selected_inds, data_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cdec0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_users = {i: np.array([]) for i in range(4)}\n",
    "\n",
    "def create_client_iid(image_list, label_list, client_num):    \n",
    "    max_y = np.argmax(label_list, axis=-1)\n",
    "    sorted_zip = sorted(zip(max_y, label_list, image_list), key=lambda x: x[0])\n",
    "    data = [(x,y) for _,y,x in sorted_zip]\n",
    "    \n",
    "    dict_users[client_num] = data\n",
    "    \n",
    "    return dict_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb1235d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function differs from all other files sincehere dataframes are being returned instead of list of values\n",
    "def initializer_income():\n",
    "    \n",
    "    x_feats = feature_set1[cols].copy().values\n",
    "    y_feats = feature_set1[['income']].copy().values\n",
    "\n",
    "    X_train_new = sc.fit_transform(x_feats)\n",
    "    y_train_new = y_feats\n",
    "\n",
    "    \n",
    "    return [X_train_new, y_train_new]\n",
    "    \n",
    "#     x_feats = feature_set1[cols].copy()\n",
    "#     y_feats = feature_set1[['income']].copy()\n",
    "\n",
    "\n",
    "#     return [x_feats, y_feats]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4278841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hetero_clients_gender(train_sets):\n",
    "    \n",
    "    data = list(zip(train_sets[0], train_sets[1]))\n",
    "    random.shuffle(data)\n",
    "    \n",
    "#     print(type(train_sets), type(train_sets[0]),type(list(list(zip(*data))[0]) ), type(data))\n",
    "    \n",
    "    train_sets[0] = list(list(zip(*data))[0])\n",
    "    train_sets[1] = list(list(zip(*data))[1])\n",
    "    \n",
    "    \n",
    "    # NON_IID\n",
    "    clients, inds, data_out1 = create_hetero_clients(train_sets[0], train_sets[1], start_client = 0, num_clients=10, initial='client') \n",
    "#     print(\"data_out1\")\n",
    "#     print(type(data_out1))\n",
    "#     print(data_out1[0])\n",
    "\n",
    "# #     IID\n",
    "#     clients = create_client_iid(train_sets[0], train_sets[1], 0)\n",
    "    \n",
    "#     clients2 = create_client_iid(train_sets[2], train_sets[3], 1)\n",
    "#     clients = {**clients, **clients2}\n",
    "    \n",
    "#     clients3 = create_client_iid(train_sets[4], train_sets[5], 2)\n",
    "#     clients = {**clients, **clients3}\n",
    "    \n",
    "#     clients4 = create_client_iid(train_sets[6], train_sets[7], 3)\n",
    "#     clients = {**clients, **clients4}\n",
    "    \n",
    "\n",
    "    \n",
    "    return clients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "715ed657",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T23:59:52.579114Z",
     "start_time": "2021-07-15T23:59:52.576244Z"
    }
   },
   "outputs": [],
   "source": [
    "# take bs = 128 for 5 clients and 10 rounds\n",
    "def batch_data(data_shard, bs=64):\n",
    "    '''Takes in a clients data shard and create a tfds object off it\n",
    "    args:\n",
    "        shard: a data, label constituting a client's data shard\n",
    "        bs:batch size\n",
    "    return:\n",
    "        tfds object'''\n",
    "#     print(type(data_shard))\n",
    "    data = []\n",
    "    label = []\n",
    "    for x in data_shard:\n",
    "        data.append(x[0])\n",
    "        label.append(x[1])\n",
    "    #seperate shard into data and labels lists\n",
    "#     data, label = zip(*data_shard)\n",
    "#     print(type(data[0][0]))\n",
    "#     print(\"data: \", data[0])\n",
    "#     print(\"label: \", label[0])\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "#     print( label[0])\n",
    "    return dataset.shuffle(len(label)).batch(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfd80728",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T23:59:54.563066Z",
     "start_time": "2021-07-15T23:59:52.773340Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#process and batch the training data for each client\n",
    "def batch_clients(clients):\n",
    "    clients_batched = dict()\n",
    "    for (client_name, data) in clients.items():\n",
    "#         print(\"data \",type(data))\n",
    "        clients_batched[client_name] = batch_data(data,126)#non-IID\n",
    "#         clients_batched[client_name] = batch_data(data,1) #IID\n",
    "    \n",
    "\n",
    "    #process and batch the test set  \n",
    "    test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))\n",
    "    \n",
    "#     test_batched\n",
    "    return clients_batched, test_batched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9dcf2ff7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T23:59:54.568193Z",
     "start_time": "2021-07-15T23:59:54.564443Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleMLP:\n",
    "    @staticmethod\n",
    "    def build(shape, classes=2 , learning_rate = 0.001, metric = \"accuracy\"):\n",
    "\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Dense(128, input_shape = (shape,)))\n",
    "#         model.add(Dense(128, Activation(\"relu\")))\n",
    "#         model.add(Dense(64, Activation(\"relu\")))\n",
    "#         model.add(Dense(32, Activation(\"relu\")))\n",
    "#         model.add(Dense(1))\n",
    "        \n",
    "        model.add(Dense(128, Activation(\"tanh\")))\n",
    "        model.add(Dense(64, Activation(\"tanh\")))\n",
    "        model.add(Dense(32, Activation(\"tanh\")))\n",
    "        model.add(Dense(1,Activation('sigmoid')))\n",
    "        \n",
    "\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f71dc14a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T00:00:30.156165Z",
     "start_time": "2021-07-16T00:00:30.152576Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = 0.005\n",
    "comms_round = 10\n",
    "loss=tf.keras.losses.BinaryCrossentropy(from_logits = False)\n",
    "\n",
    "metrics = ['binary_accuracy']\n",
    "\n",
    "optimizer = SGD(learning_rate=lr, \n",
    "                decay=lr / comms_round, \n",
    "                momentum=0.5\n",
    "               )     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eebf9376",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T00:00:30.412962Z",
     "start_time": "2021-07-16T00:00:30.399142Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def weight_scalling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    #get the bs\n",
    "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    #first calculate the total training data points across clinets\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
    "    # get the total number of data points held by a client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
    "    return local_count/global_count\n",
    "\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    '''function for scaling a models weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final\n",
    "\n",
    "\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "#     print(len(scaled_weight_list))\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "#         print(len(grad_list_tuple))\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "\n",
    "\n",
    "def test_model(X_test, Y_test,  model, comm_round):\n",
    "    \n",
    "#     cce = tf.keras.losses.BinaryCrossentropy(from_logits = True)\n",
    "#     cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    logits = model.predict(X_test)\n",
    "\n",
    "    score = global_model.evaluate(X_test, y_test, verbose=0)\n",
    "    acc = score[1] ; loss = score[0]\n",
    "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
    "\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9dd79b15",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'female_quantiles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m final_female  \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m): \n\u001b[1;32m----> 4\u001b[0m     female_fts \u001b[38;5;241m=\u001b[39m female_features[female_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfnlwgt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mfemale_quantiles\u001b[49m[i]]\n\u001b[0;32m      5\u001b[0m     female_fts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfnlwgt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m quantiles[i]\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(female_fts\u001b[38;5;241m.\u001b[39mindex)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'female_quantiles' is not defined"
     ]
    }
   ],
   "source": [
    "female_features =  feature_set1[feature_set1['gender_Female']==1]\n",
    "final_female  = pd.DataFrame()\n",
    "for i in range(10): \n",
    "    female_fts = female_features[female_features[\"fnlwgt\"]<= female_quantiles[i]]\n",
    "    female_fts[\"fnlwgt\"] = quantiles[i]\n",
    "    \n",
    "    \n",
    "    print(female_fts.index)\n",
    "    female_features = female_features.drop(female_fts.index)\n",
    "    print(\"\\n\")\n",
    "    print((female_features.shape), final_female.shape)\n",
    "    final_female = pd.concat([final_female, female_fts])\n",
    "    print(female_fts[\"fnlwgt\"], female_quantiles[i])\n",
    "    \n",
    "male_features = feature_set1[feature_set1['gender_Male']==1] \n",
    "final_male  = pd.DataFrame()\n",
    "for i in range(10): \n",
    "    male_fts = male_features[male_features[\"fnlwgt\"]<= male_quantiles[i]]\n",
    "    male_fts[\"fnlwgt\"] = quantiles[i]\n",
    "    \n",
    "    \n",
    "#     print(male_fts.index)\n",
    "    male_features = male_features.drop(male_fts.index)\n",
    "#     print(\"\\n\")\n",
    "#     print((male_features.shape), final_male.shape)\n",
    "    final_male = pd.concat([final_male, male_fts])\n",
    "    \n",
    "    \n",
    "    repaired_data = pd.DataFrame()\n",
    "repaired_data = pd.concat([final_male, final_female])\n",
    "repaired_data = shuffle(repaired_data)\n",
    "sensitive_atrributes =  [ 'age',\n",
    "    'race_Amer-Indian-Eskimo',\n",
    "    'race_Asian-Pac-Islander',\n",
    "    'race_Black',\n",
    "    'race_Other',\n",
    "    'race_White',\n",
    "    'gender_Female',\n",
    "    'gender_Male']\n",
    "for i in sensitive_atrributes: \n",
    "    repaired_data[i] = 0\n",
    "    \n",
    "    repaired_data = repaired_data.drop('Unnamed: 0', axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4aef41d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T00:01:47.756094Z",
     "start_time": "2021-07-16T00:00:30.586083Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_out1\n",
      "<class 'list'>\n",
      "(array([ 0.91646482, -0.04649177, -0.15963972,  1.21496536, -0.03523147,\n",
      "       -0.14593585, -0.2175757 , -0.03492533,  2.52626483, -0.02771973,\n",
      "       -0.92136273, -0.11269381, -0.70156267, -0.17891851, -0.179527  ,\n",
      "       -0.24508517,  2.76555523, -0.01678105, -0.37649823, -0.37618802,\n",
      "       -0.17815534, -0.21117047, -0.2554779 , -0.33653279, -0.07082154,\n",
      "       -0.38135638, -0.14300071, -0.35746063, -0.17351455, -0.22532901,\n",
      "       -0.82412796,  1.69528279, -0.17654328, -0.42836007, -0.34117932,\n",
      "       -0.22419805, -0.24449789,  5.78410825, -0.26401111, -0.0159999 ,\n",
      "       -1.50719061, -0.18939262, -0.29443044, -0.20406156, -0.02086318,\n",
      "       -0.09870466, -0.17792583, -0.32481669, -0.09115626,  0.41017233,\n",
      "       -0.70282207,  0.70282207, -0.13427492, -0.02426908, -0.0595346 ,\n",
      "       -0.04962853, -0.03986597, -0.05456777, -0.04501058, -0.02994265,\n",
      "       -0.05732963, -0.04831574, -0.02771973, -0.0639213 , -0.031609  ,\n",
      "       -0.04296634, -0.03888801,  0.        , -0.02086318, -0.02426908,\n",
      "       -0.02024   , -0.05456777, -0.03615183, -0.02771973, -0.04414598,\n",
      "       -0.04778062, -0.04206007, -0.02086318, -0.14055874, -0.03319212,\n",
      "       -0.02205688, -0.03078706, -0.07959772, -0.04206007, -0.03615183,\n",
      "       -0.06060742, -0.02086318, -0.04804892, -0.03720136, -0.02580434,\n",
      "       -0.02373533,  0.33695676, -0.04175364, -0.02205688]), array([1], dtype=int64))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 32\u001b[0m\n\u001b[0;32m     28\u001b[0m     clients \u001b[38;5;241m=\u001b[39m get_hetero_clients_gender(\u001b[38;5;28mlist\u001b[39m(train_sets) )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#     clients = get_hetero_clients_gender_race(list(train_sets) )\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#     print(\"client0: \", clients[0])\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     clients_batched, test_batched \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_clients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#     print(\"client batched 0: \", clients_batched[1])\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# get the global model's weights - will serve as the initial weights for all local models\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     global_weights \u001b[38;5;241m=\u001b[39m global_model\u001b[38;5;241m.\u001b[39mget_weights()\n",
      "Cell \u001b[1;32mIn[26], line 6\u001b[0m, in \u001b[0;36mbatch_clients\u001b[1;34m(clients)\u001b[0m\n\u001b[0;32m      3\u001b[0m     clients_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (client_name, data) \u001b[38;5;129;01min\u001b[39;00m clients\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#         print(\"data \",type(data))\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m         clients_batched[client_name] \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m126\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#non-IID\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#         clients_batched[client_name] = batch_data(data,1) #IID\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m#process and batch the test set  \u001b[39;00m\n\u001b[0;32m     11\u001b[0m     test_batched \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices((X_test, y_test))\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;28mlen\u001b[39m(y_test))\n",
      "Cell \u001b[1;32mIn[41], line 13\u001b[0m, in \u001b[0;36mbatch_data\u001b[1;34m(data_shard, bs)\u001b[0m\n\u001b[0;32m     11\u001b[0m     label \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data_shard:\n\u001b[1;32m---> 13\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     14\u001b[0m         label\u001b[38;5;241m.\u001b[39mappend(x[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m#seperate shard into data and labels lists\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#     data, label = zip(*data_shard)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#     print(type(data[0][0]))\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#     print(\"data: \", data[0])\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#     print(\"label: \", label[0])\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#initialize global model\n",
    "smlp_global = SimpleMLP()\n",
    "global_model = smlp_global.build(X_train.shape[1] ,classes=2)\n",
    "global_model.compile(optimizer=optimizer, loss=loss, metrics=metrics) \n",
    "\n",
    "\n",
    "client_names = [0, 1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "\n",
    "global_loss_list = []\n",
    "global_freq_list = []\n",
    "global_acc_list = []\n",
    "client_loss = {i: np.array([]) for i in range(len(client_names))}\n",
    "client_accuracy = {i: np.array([]) for i in range(len(client_names))}\n",
    "client_frequency = {i: np.array([]) for i in range(len(client_names))}\n",
    "\n",
    "train_sets = initializer_income()\n",
    "# train_sets = initializer_income_race_gender()\n",
    "\n",
    "#commence global training loop\n",
    "for comm_round in range(comms_round):\n",
    "    epoch_freq = 0\n",
    "#     clients = create_clients(X_train, y_train, num_clients=15, initial='client')\n",
    "#     clients = get_hetero_clients()\n",
    "#     print(type(train_sets[0]))\n",
    "\n",
    "#     print(train_sets[0]['age'])\n",
    "    clients = get_hetero_clients_gender(list(train_sets) )\n",
    "#     clients = get_hetero_clients_gender_race(list(train_sets) )\n",
    "#     print(\"client0: \", clients[0])\n",
    "\n",
    "    clients_batched, test_batched = batch_clients(clients)\n",
    "#     print(\"client batched 0: \", clients_batched[1])\n",
    "    # get the global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "    \n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    #randomize client data - using keys\n",
    "    client_names= list(clients_batched.keys())\n",
    "#     print(client_names)\n",
    "    random.shuffle(client_names)\n",
    "    \n",
    "    #loop through each client and create new local model\n",
    "    for client in client_names:\n",
    "        smlp_local = SimpleMLP()\n",
    "        local_model = smlp_local.build(X_train.shape[1],classes=2)\n",
    "        local_model.compile(loss=loss, \n",
    "                      optimizer=optimizer, \n",
    "                      metrics=metrics)\n",
    "        \n",
    "        #set local model weight to the weight of the global model\n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        #fit local model with client's data\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"//////////////////////////////\")\n",
    "#         example = list(clients_batched[client].as_numpy_iterator())\n",
    "#         example[0]['age']\n",
    "        \n",
    "        print(\"clients_batched: \", clients_batched[client])\n",
    "#                 print(\"clients: \", (clients[0][0]) )\n",
    "        \n",
    "        history = local_model.fit(clients_batched[client], epochs=1, verbose=1)\n",
    "        \n",
    "#         get client acc, loss\n",
    "#         print(client)\n",
    "        if(len(client_loss[client])== 0):\n",
    "            client_loss[client] = [history.history['loss'][0]]\n",
    "            client_accuracy[client] = [history.history['binary_accuracy'][0]]\n",
    "            client_frequency[client] = [len(clients_batched[client])]\n",
    "        \n",
    "        else:\n",
    "            client_loss[client] = np.append(client_loss[client], (history.history['loss'][0]))\n",
    "            client_frequency[client] = np.append(client_frequency[client], len(clients_batched[client]))\n",
    "            client_accuracy[client] = np.append(client_accuracy[client], (history.history['binary_accuracy'][0]))\n",
    "        \n",
    "        epoch_freq += len(clients_batched[client])\n",
    "        \n",
    "        print(\"client\", client, \"loss -->\" ,client_loss[client], \"freq->\", client_frequency[client], \"accuracy->\", client_accuracy[client])\n",
    "        \n",
    "        #scale the model weights and add to list\n",
    "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "        \n",
    "        #clear session to free memory after each communication round\n",
    "        K.clear_session()\n",
    "        \n",
    "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    \n",
    "    #update global model \n",
    "    global_model.set_weights(average_weights)\n",
    "    \n",
    "    global_freq_list.append(epoch_freq)\n",
    "    epoch_freq = 0\n",
    "    \n",
    "\n",
    "#     test global model and print out metrics after each communications round\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)\n",
    "        global_loss_list.append(global_loss)\n",
    "        global_acc_list.append(global_acc)\n",
    "        \n",
    "        print(\"global_loss_list: \", global_loss_list )\n",
    "        print(\"global_acc_list: \", global_acc_list )\n",
    "        print(\"global_freq_list: \", global_freq_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7eba6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>income</th>\n",
       "      <th>marital-status_Divorced</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_Portugal</th>\n",
       "      <th>native-country_Puerto-Rico</th>\n",
       "      <th>native-country_Scotland</th>\n",
       "      <th>native-country_South</th>\n",
       "      <th>native-country_Taiwan</th>\n",
       "      <th>native-country_Thailand</th>\n",
       "      <th>native-country_Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_United-States</th>\n",
       "      <th>native-country_Vietnam</th>\n",
       "      <th>native-country_Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38914</td>\n",
       "      <td>25</td>\n",
       "      <td>178960</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5303</td>\n",
       "      <td>31</td>\n",
       "      <td>158672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37481</td>\n",
       "      <td>17</td>\n",
       "      <td>25051</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33175</td>\n",
       "      <td>50</td>\n",
       "      <td>231196</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30045</td>\n",
       "      <td>22</td>\n",
       "      <td>187052</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39068</th>\n",
       "      <td>43280</td>\n",
       "      <td>60</td>\n",
       "      <td>190682</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39069</th>\n",
       "      <td>18495</td>\n",
       "      <td>64</td>\n",
       "      <td>177825</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1055</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39070</th>\n",
       "      <td>27726</td>\n",
       "      <td>49</td>\n",
       "      <td>192323</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39071</th>\n",
       "      <td>25866</td>\n",
       "      <td>17</td>\n",
       "      <td>156736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39072</th>\n",
       "      <td>21093</td>\n",
       "      <td>69</td>\n",
       "      <td>164102</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39073 rows Ã— 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  age  fnlwgt  education  educational-num  capital-gain  \\\n",
       "0           38914   25  178960        1.0                7             0   \n",
       "1            5303   31  158672        1.0                7             0   \n",
       "2           37481   17   25051        0.0                6             0   \n",
       "3           33175   50  231196        9.0               13             0   \n",
       "4           30045   22  187052       11.0                9             0   \n",
       "...           ...  ...     ...        ...              ...           ...   \n",
       "39068       43280   60  190682        8.0               11             0   \n",
       "39069       18495   64  177825       11.0                9          1055   \n",
       "39070       27726   49  192323       11.0                9             0   \n",
       "39071       25866   17  156736        0.0                6             0   \n",
       "39072       21093   69  164102       11.0                9             0   \n",
       "\n",
       "       capital-loss  hours-per-week  income  marital-status_Divorced  ...  \\\n",
       "0                 0              40       0                        0  ...   \n",
       "1                 0              38       0                        0  ...   \n",
       "2                 0              16       0                        0  ...   \n",
       "3                 0              50       0                        1  ...   \n",
       "4                 0              40       0                        0  ...   \n",
       "...             ...             ...     ...                      ...  ...   \n",
       "39068             0              37       0                        0  ...   \n",
       "39069             0              40       0                        0  ...   \n",
       "39070             0              40       0                        0  ...   \n",
       "39071             0              12       0                        0  ...   \n",
       "39072             0              40       1                        1  ...   \n",
       "\n",
       "       native-country_Portugal  native-country_Puerto-Rico  \\\n",
       "0                            0                           0   \n",
       "1                            0                           1   \n",
       "2                            0                           0   \n",
       "3                            0                           0   \n",
       "4                            0                           0   \n",
       "...                        ...                         ...   \n",
       "39068                        0                           0   \n",
       "39069                        0                           0   \n",
       "39070                        0                           0   \n",
       "39071                        0                           0   \n",
       "39072                        0                           0   \n",
       "\n",
       "       native-country_Scotland  native-country_South  native-country_Taiwan  \\\n",
       "0                            0                     0                      0   \n",
       "1                            0                     0                      0   \n",
       "2                            0                     0                      0   \n",
       "3                            0                     0                      0   \n",
       "4                            0                     0                      0   \n",
       "...                        ...                   ...                    ...   \n",
       "39068                        0                     0                      0   \n",
       "39069                        0                     0                      0   \n",
       "39070                        0                     0                      0   \n",
       "39071                        0                     0                      0   \n",
       "39072                        0                     0                      0   \n",
       "\n",
       "       native-country_Thailand  native-country_Trinadad&Tobago  \\\n",
       "0                            0                               0   \n",
       "1                            0                               0   \n",
       "2                            0                               0   \n",
       "3                            0                               0   \n",
       "4                            0                               0   \n",
       "...                        ...                             ...   \n",
       "39068                        0                               0   \n",
       "39069                        0                               0   \n",
       "39070                        0                               0   \n",
       "39071                        0                               0   \n",
       "39072                        0                               0   \n",
       "\n",
       "       native-country_United-States  native-country_Vietnam  \\\n",
       "0                                 1                       0   \n",
       "1                                 0                       0   \n",
       "2                                 1                       0   \n",
       "3                                 1                       0   \n",
       "4                                 1                       0   \n",
       "...                             ...                     ...   \n",
       "39068                             1                       0   \n",
       "39069                             1                       0   \n",
       "39070                             1                       0   \n",
       "39071                             1                       0   \n",
       "39072                             1                       0   \n",
       "\n",
       "       native-country_Yugoslavia  \n",
       "0                              0  \n",
       "1                              0  \n",
       "2                              0  \n",
       "3                              0  \n",
       "4                              0  \n",
       "...                          ...  \n",
       "39068                          0  \n",
       "39069                          0  \n",
       "39070                          0  \n",
       "39071                          0  \n",
       "39072                          0  \n",
       "\n",
       "[39073 rows x 95 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_set1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2208b3c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats \n",
    "\n",
    "pdfs = []\n",
    "for i in list(client_accuracy.keys()):\n",
    "    print(\"client_\",i, \" loss: \", client_loss[i],\"freq: \", client_frequency[i], \"acc\", client_accuracy[i])\n",
    "    \n",
    "#     norm\n",
    "#     mean, std = stats.norm.fit(client_loss[i])\n",
    "#     pdf =  stats.norm.pdf(client_loss[i], mean, std)\n",
    "\n",
    "#     df_mean = np.mean(client_frequency[i])\n",
    "#     df_std = np.std(client_frequency[i])\n",
    "#     pdf = stats.norm.pdf(client_frequency[i], df_mean, df_std)\n",
    "    \n",
    "#     lognorm\n",
    "#     shape, loc, scale = stats.lognorm.fit(client_loss[i])\n",
    "#     pdf = stats.lognorm.pdf(client_loss[i], shape, loc, scale)\n",
    "\n",
    "#     beta\n",
    "#     beta_params = stats.beta.fit(client_loss[i])\n",
    "#     pdf = stats.beta.pdf(client_loss[i], beta_params[0], beta_params[1], beta_params[2], beta_params[3])\n",
    "\n",
    "#     burr\n",
    "#     burr_params = stats.burr.fit(client_loss[i])\n",
    "#     pdf = stats.burr.pdf(client_loss[i], burr_params[0], burr_params[1], burr_params[2], burr_params[3])\n",
    "\n",
    "    \n",
    "#  gamma\n",
    "    shape, loc, scale = stats.gamma.fit(client_loss[i])\n",
    "    pdf = stats.gamma.cdf(client_loss[i], shape, loc=loc, scale=scale)\n",
    "    \n",
    "    pdfs.append(pdf)\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    ax = sns.displot( x = pdf, kind = \"kde\", height=10, aspect=2,\n",
    "                linewidth = 5 )\n",
    "    plt.ylabel(\"LOSS\")\n",
    "#     plt.xlabel(\"Loss\")\n",
    "    plt.xlabel(\"CDF\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739fe197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats \n",
    "\n",
    "for i in list(client_accuracy.keys()):\n",
    "    print(\"client_\",i, \"\\nloss: \", client_loss[i],\"\\nfreq: \", client_frequency[i], \"\\nacc\", client_accuracy[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cb1f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"_GAMMA_\")\n",
    "\n",
    "cdfs_loss_gamma = []\n",
    "cdfs_acc_gamma = []\n",
    "cdfs_freq_gamma = []\n",
    "for i in list(client_accuracy.keys()):\n",
    "#     print(\"client_\",i, \"\\nloss: \", client_loss[i],\"\\nfreq: \", client_frequency[i], \"\\nacc\", client_accuracy[i])\n",
    "\n",
    "    shape, loc, scale = stats.gamma.fit(client_loss[i])\n",
    "    cdf_loss_gamma =  stats.gamma.cdf(client_loss[i], shape, loc, scale)\n",
    "    cdfs_loss_gamma.append(cdf_loss_gamma)\n",
    "\n",
    "    shape, loc, scale= stats.gamma.fit(client_accuracy[i])\n",
    "    cdf_acc_gamma = stats.gamma.cdf(client_accuracy[i],shape, loc, scale)\n",
    "    cdfs_acc_gamma.append(cdf_acc_gamma)\n",
    "\n",
    "    shape, loc, scale  = stats.gamma.fit(client_frequency[i])\n",
    "    cdf_freq_gamma = stats.gamma.cdf(client_frequency[i],shape, loc, scale )\n",
    "    cdfs_freq_gamma.append(cdf_freq_gamma)\n",
    "\n",
    "\n",
    "print(\"ACCURACY\")\n",
    "for i in range(len(cdfs_acc_gamma)):\n",
    "    for j in range(i+1,len(cdfs_acc_gamma)):\n",
    "        print(i,\" and \", j , stats.wasserstein_distance(cdfs_acc_gamma[i], cdfs_acc_gamma[j]))\n",
    "\n",
    "\n",
    "shape, loc, scale = stats.gamma.fit(global_acc_list)\n",
    "cdf_global_acc_gamma =  stats.gamma.cdf(global_acc_list, shape, loc, scale)\n",
    "\n",
    "for i in range(len(cdfs_acc_gamma)):\n",
    "    print(i,\" and global: \" , stats.wasserstein_distance(cdfs_acc_gamma[i], cdf_global_acc_gamma))\n",
    "\n",
    "print(\"LOSS\")\n",
    "for i in range(len(cdfs_loss_gamma)):\n",
    "    for j in range(i+1,len(cdfs_loss_gamma)):\n",
    "        print(i,\" and \", j , stats.wasserstein_distance(cdfs_loss_gamma[i], cdfs_loss_gamma[j]))\n",
    "\n",
    "\n",
    "shape, loc, scale = stats.gamma.fit(global_loss_list)\n",
    "cdf_global_loss_gamma =  stats.gamma.cdf(global_loss_list, shape, loc, scale)\n",
    "\n",
    "for i in range(len(cdfs_loss_gamma)):\n",
    "    print(i,\" and global: \" , stats.wasserstein_distance(cdfs_loss_gamma[i], cdf_global_loss_gamma))\n",
    "\n",
    "print(\"FREQUENCY\")\n",
    "for i in range(len(cdfs_freq_gamma)):\n",
    "    for j in range(i+1,len(cdfs_freq_gamma)):\n",
    "        print(i,\" and \", j , stats.wasserstein_distance(cdfs_freq_gamma[i], cdfs_freq_gamma[j]))\n",
    "\n",
    "\n",
    "shape, loc, scale = stats.gamma.fit(global_freq_list)\n",
    "cdf_global_freq_gamma =  stats.gamma.cdf(global_freq_list,shape, loc, scale)\n",
    "\n",
    "for i in range(len(cdfs_freq_gamma)):\n",
    "    print(i,\" and global: \" , stats.wasserstein_distance(cdfs_freq_gamma[i], cdf_global_freq_gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30032e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(len(pdfs_loss_gamma)):\n",
    "#     print(i,\" and global: \" , stats.wasserstein_distance(pdfs_loss_gamma[i], pdf_global_loss_gamma))\n",
    "#     M = ot.dist(pdfs_loss_gamma[i], pdf_global_loss_gamma, metric='euclidean')\n",
    "#     W = ot.emd2(pdfs_loss_gamma[i], pdf_global_loss_gamma, M)\n",
    "#     print(\"actual \", W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c29d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"_NORM_\")\n",
    "\n",
    "pdfs_loss_norm = []\n",
    "pdfs_acc_norm = []\n",
    "pdfs_freq_norm = []\n",
    "for i in list(client_accuracy.keys()):\n",
    "#     print(\"client_\",i, \" loss: \", client_loss[i],\"freq: \", client_frequency[i], \"acc\", client_accuracy[i])\n",
    "    \n",
    "    mean, std = stats.norm.fit(client_loss[i])\n",
    "    pdf_loss_norm =  stats.norm.pdf(client_loss[i], mean, std)\n",
    "    pdfs_loss_norm.append(pdf_loss_norm)\n",
    "\n",
    "    mean, std = stats.norm.fit(client_accuracy[i])\n",
    "    pdf_acc_norm = stats.norm.pdf(client_accuracy[i],mean, std )\n",
    "    pdfs_acc_norm.append(pdf_acc_norm)\n",
    "    \n",
    "    mean, std  = stats.norm.fit(client_frequency[i])\n",
    "    pdf_freq_norm = stats.norm.pdf(client_frequency[i], mean, std )\n",
    "    pdfs_freq_norm.append(pdf_freq_norm)\n",
    "    \n",
    "    \n",
    "print(\"ACCURACY\")\n",
    "for i in range(len(pdfs_acc_norm)):\n",
    "    for j in range(i+1,len(pdfs_acc_norm)):\n",
    "        print(i,\" and \", j , stats.wasserstein_distance(pdfs_acc_norm[i], pdfs_acc_norm[j]))\n",
    "        \n",
    "        \n",
    "mean, std = stats.norm.fit(global_acc_list)\n",
    "pdf_global_acc_norm =  stats.norm.pdf(global_acc_list, mean, std)\n",
    "\n",
    "for i in range(len(pdfs_acc_norm)):\n",
    "    print(i,\" and global: \" , stats.wasserstein_distance(pdfs_acc_norm[i], pdf_global_acc_norm))\n",
    "    \n",
    "print(\"LOSS\")\n",
    "for i in range(len(pdfs_loss_norm)):\n",
    "    for j in range(i+1,len(pdfs_loss_norm)):\n",
    "        print(i,\" and \", j , stats.wasserstein_distance(pdfs_loss_norm[i], pdfs_loss_norm[j]))\n",
    "        \n",
    "        \n",
    "mean, std = stats.norm.fit(global_loss_list)\n",
    "pdf_global_loss_norm =  stats.norm.pdf(global_loss_list, mean, std)\n",
    "\n",
    "for i in range(len(pdfs_loss_norm)):\n",
    "    print(i,\" and global: \" , stats.wasserstein_distance(pdfs_loss_norm[i], pdf_global_loss_norm))\n",
    "    \n",
    "print(\"FREQUENCY\")\n",
    "for i in range(len(pdfs_freq_norm)):\n",
    "    for j in range(i+1,len(pdfs_freq_norm)):\n",
    "        print(i,\" and \", j , stats.wasserstein_distance(pdfs_freq_norm[i], pdfs_freq_norm[j]))\n",
    "        \n",
    "        \n",
    "mean, std = stats.norm.fit(global_freq_list)\n",
    "pdf_global_freq_norm =  stats.norm.pdf(global_freq_list, mean, std)\n",
    "\n",
    "for i in range(len(pdfs_freq_norm)):\n",
    "    print(i,\" and global: \" , stats.wasserstein_distance(pdfs_freq_norm[i], pdf_global_freq_norm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a177e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"_LOGNORM_\")\n",
    "\n",
    "pdfs_loss = []\n",
    "pdfs_acc = []\n",
    "pdfs_freq = []\n",
    "for i in list(client_accuracy.keys()):\n",
    "#     print(\"client_\",i, \"\\nloss: \", client_loss[i],\"\\nfreq: \", client_frequency[i], \"\\nacc\", client_accuracy[i])\n",
    "\n",
    "    shape, loc, scale = stats.lognorm.fit(client_loss[i])\n",
    "    pdf_loss =  stats.lognorm.pdf(client_loss[i], shape, loc, scale)\n",
    "    pdfs_loss.append(pdf_loss)\n",
    "\n",
    "    shape, loc, scale= stats.lognorm.fit(client_accuracy[i])\n",
    "    pdf_acc = stats.lognorm.pdf(client_accuracy[i],shape, loc, scale)\n",
    "    pdfs_acc.append(pdf_acc)\n",
    "\n",
    "    shape, loc, scale  = stats.lognorm.fit(client_frequency[i])\n",
    "    pdf_freq = stats.lognorm.pdf(client_frequency[i],shape, loc, scale )\n",
    "    pdfs_freq.append(pdf_freq)\n",
    "\n",
    "\n",
    "print(\"ACCURACY\")\n",
    "for i in range(len(pdfs_acc)):\n",
    "    for j in range(i+1,len(pdfs_acc)):\n",
    "        print(i,\" and \", j , stats.wasserstein_distance(pdfs_acc[i], pdfs_acc[j]))\n",
    "\n",
    "\n",
    "shape, loc, scale = stats.lognorm.fit(global_acc_list)\n",
    "pdf_global_acc =  stats.lognorm.pdf(global_acc_list, shape, loc, scale)\n",
    "\n",
    "for i in range(len(pdfs_acc)):\n",
    "    print(i,\" and global: \" , stats.wasserstein_distance(pdfs_acc[i], pdf_global_acc))\n",
    "\n",
    "print(\"LOSS\")\n",
    "for i in range(len(pdfs_loss)):\n",
    "    for j in range(i+1,len(pdfs_loss)):\n",
    "        print(i,\" and \", j , stats.wasserstein_distance(pdfs_loss[i], pdfs_loss[j]))\n",
    "\n",
    "\n",
    "shape, loc, scale = stats.lognorm.fit(global_loss_list)\n",
    "pdf_global_loss =  stats.lognorm.pdf(global_loss_list, shape, loc, scale)\n",
    "\n",
    "for i in range(len(pdfs_loss)):\n",
    "    print(i,\" and global: \" , stats.wasserstein_distance(pdfs_loss[i], pdf_global_loss))\n",
    "\n",
    "print(\"FREQUENCY\")\n",
    "for i in range(len(pdfs_freq)):\n",
    "    for j in range(i+1,len(pdfs_freq)):\n",
    "        print(i,\" and \", j , stats.wasserstein_distance(pdfs_freq[i], pdfs_freq[j]))\n",
    "\n",
    "\n",
    "shape, loc, scale = stats.lognorm.fit(global_freq_list)\n",
    "pdf_global_freq =  stats.lognorm.pdf(global_freq_list,shape, loc, scale)\n",
    "\n",
    "for i in range(len(pdfs_freq)):\n",
    "    print(i,\" and global: \" , stats.wasserstein_distance(pdfs_freq[i], pdf_global_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10261017",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"_BETA_\")\n",
    "\n",
    "pdfs_loss = []\n",
    "pdfs_acc = []\n",
    "pdfs_freq = []\n",
    "for i in list(client_accuracy.keys()):\n",
    "\n",
    "    beta_params = stats.beta.fit(client_loss[i])\n",
    "    pdf_loss =  stats.beta.pdf(client_loss[i],beta_params[0], beta_params[1], beta_params[2], beta_params[3])\n",
    "    pdfs_loss.append(pdf_loss)\n",
    "\n",
    "    beta_params = stats.beta.fit(client_accuracy[i])\n",
    "    pdf_acc = stats.beta.pdf(client_accuracy[i],beta_params[0], beta_params[1], beta_params[2], beta_params[3])\n",
    "    pdfs_acc.append(pdf_acc)\n",
    "\n",
    "    beta_params  = stats.beta.fit(client_frequency[i])\n",
    "    pdf_freq = stats.beta.pdf(client_frequency[i],beta_params[0], beta_params[1], beta_params[2], beta_params[3])\n",
    "    pdfs_freq.append(pdf_freq)\n",
    "\n",
    "\n",
    "print(\"ACCURACY\")\n",
    "for i in range(len(pdfs_acc)):\n",
    "    for j in range(i+1,len(pdfs_acc)):\n",
    "        print(i,\" and \", j , stats.wasserstein_distance(pdfs_acc[i], pdfs_acc[j]))\n",
    "\n",
    "\n",
    "beta_params = stats.beta.fit(global_acc_list)\n",
    "pdf_global_acc =  stats.beta.pdf(global_acc_list, beta_params[0], beta_params[1], beta_params[2], beta_params[3])\n",
    "\n",
    "for i in range(len(pdfs_acc)):\n",
    "    print(i,\" and global: \" , stats.wasserstein_distance(pdfs_acc[i], pdf_global_acc))\n",
    "\n",
    "print(\"LOSS\")\n",
    "for i in range(len(pdfs_loss)):\n",
    "    for j in range(i+1,len(pdfs_loss)):\n",
    "        print(i,\" and \", j , stats.wasserstein_distance(pdfs_loss[i], pdfs_loss[j]))\n",
    "\n",
    "\n",
    "beta_params = stats.beta.fit(global_loss_list)\n",
    "pdf_global_loss =  stats.beta.pdf(global_loss_list, beta_params[0], beta_params[1], beta_params[2], beta_params[3])\n",
    "\n",
    "for i in range(len(pdfs_loss)):\n",
    "    print(i,\" and global: \" , stats.wasserstein_distance(pdfs_loss[i], pdf_global_loss))\n",
    "\n",
    "print(\"FREQUENCY\")\n",
    "for i in range(len(pdfs_freq)):\n",
    "    for j in range(i+1,len(pdfs_freq)):\n",
    "        print(i,\" and \", j , stats.wasserstein_distance(pdfs_freq[i], pdfs_freq[j]))\n",
    "\n",
    "\n",
    "beta_params = stats.beta.fit(global_freq_list)\n",
    "pdf_global_freq =  stats.beta.pdf(global_freq_list,beta_params[0], beta_params[1], beta_params[2], beta_params[3])\n",
    "\n",
    "for i in range(len(pdfs_freq)):\n",
    "    print(i,\" and global: \" , stats.wasserstein_distance(pdfs_freq[i], pdf_global_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3cc154",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"_BURR_\")\n",
    "\n",
    "pdfs_loss = []\n",
    "pdfs_acc = []\n",
    "pdfs_freq = []\n",
    "for i in list(client_accuracy.keys()):\n",
    "\n",
    "    params = stats.burr.fit(client_loss[i])\n",
    "    pdf_loss =  stats.burr.pdf(client_loss[i],params[0], params[1], params[2], params[3])\n",
    "    pdfs_loss.append(pdf_loss)\n",
    "\n",
    "    beta_params = stats.burr.fit(client_accuracy[i])\n",
    "    pdf_acc = stats.burr.pdf(client_accuracy[i],beta_params[0], beta_params[1], beta_params[2], beta_params[3])\n",
    "    pdfs_acc.append(pdf_acc)\n",
    "\n",
    "    beta_params  = stats.burr.fit(client_frequency[i])\n",
    "    pdf_freq = stats.burr.pdf(client_frequency[i],beta_params[0], beta_params[1], beta_params[2], beta_params[3])\n",
    "    pdfs_freq.append(pdf_freq)\n",
    "\n",
    "\n",
    "print(\"ACCURACY\")\n",
    "for i in range(len(pdfs_acc)):\n",
    "    for j in range(i+1,len(pdfs_acc)):\n",
    "        print(i,\" and \", j , stats.wasserstein_distance(pdfs_acc[i], pdfs_acc[j]))\n",
    "\n",
    "\n",
    "beta_params = stats.burr.fit(global_acc_list)\n",
    "pdf_global_acc =  stats.burr.pdf(global_acc_list, beta_params[0], beta_params[1], beta_params[2], beta_params[3])\n",
    "\n",
    "for i in range(len(pdfs_acc)):\n",
    "    print(i,\" and global: \" , stats.wasserstein_distance(pdfs_acc[i], pdf_global_acc))\n",
    "\n",
    "print(\"LOSS\")\n",
    "for i in range(len(pdfs_loss)):\n",
    "    for j in range(i+1,len(pdfs_loss)):\n",
    "        print(i,\" and \", j , stats.wasserstein_distance(pdfs_loss[i], pdfs_loss[j]))\n",
    "\n",
    "\n",
    "beta_params = stats.burr.fit(global_loss_list)\n",
    "pdf_global_loss =  stats.burr.pdf(global_loss_list, beta_params[0], beta_params[1], beta_params[2], beta_params[3])\n",
    "\n",
    "for i in range(len(pdfs_loss)):\n",
    "    print(i,\" and global: \" , stats.wasserstein_distance(pdfs_loss[i], pdf_global_loss))\n",
    "\n",
    "print(\"FREQUENCY\")\n",
    "for i in range(len(pdfs_freq)):\n",
    "    for j in range(i+1,len(pdfs_freq)):\n",
    "        print(i,\" and \", j , stats.wasserstein_distance(pdfs_freq[i], pdfs_freq[j]))\n",
    "\n",
    "\n",
    "beta_params = stats.burr.fit(global_freq_list)\n",
    "pdf_global_freq =  stats.burr.pdf(global_freq_list,beta_params[0], beta_params[1], beta_params[2], beta_params[3])\n",
    "\n",
    "for i in range(len(pdfs_freq)):\n",
    "    print(i,\" and global: \" , stats.wasserstein_distance(pdfs_freq[i], pdf_global_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd01c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(client_loss.keys()):\n",
    "#     if(i== len(list(client_loss.keys()))-1):\n",
    "#         continue\n",
    "    print(\"client_\",i, \"loss: \", client_loss[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650336b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(client_loss.keys()):\n",
    "#     if(i== len(list(client_loss.keys()))-1):\n",
    "#         continue\n",
    "    print(\"client_\",i, \"freq: \", client_frequency[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a70793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pdfs)):\n",
    "    for j in range(i+1,len(pdfs)):\n",
    "        print(i,\" and \", j , stats.wasserstein_distance(pdfs[i], pdfs[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2620a6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # gamma\n",
    "shape, loc, scale = stats.gamma.fit(global_freq_list)\n",
    "pdf_global = stats.gamma.pdf(global_freq_list, shape, loc=loc, scale=scale)\n",
    "\n",
    "# df_mean = np.mean(global_loss_list)\n",
    "# df_std = np.std(global_loss_list)\n",
    "# pdf_global = stats.norm.pdf(global_loss_list, df_mean, df_std)\n",
    "\n",
    "# mean, std = stats.norm.fit(global_freq_list)\n",
    "# pdf_global =  stats.norm.pdf(global_freq_list, mean, std)\n",
    "\n",
    "# lognorm\n",
    "# shape, loc, scale = stats.lognorm.fit(global_loss_list)\n",
    "# pdf_global = stats.lognorm.pdf(global_loss_list, shape, loc, scale)\n",
    "\n",
    "# global_acc_list\n",
    "# global_freq_list\n",
    "\n",
    "# burr\n",
    "# burr_params = stats.beta.fit(global_freq_list)\n",
    "# pdf_global = stats.beta.pdf(global_freq_list, burr_params[0], burr_params[1], burr_params[2], burr_params[3])\n",
    "\n",
    "for i in range(len(pdfs)):\n",
    "    print(i,\" and global: \" , stats.wasserstein_distance(pdfs[i], pdf_global))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de13219",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8879a3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fitter import Fitter, get_common_distributions, get_distributions\n",
    "\n",
    "# for i in list(client_loss.keys()):\n",
    "#     height = client_loss[i]\n",
    "\n",
    "#     f = Fitter(height,\n",
    "#                distributions=['gamma',\n",
    "#                               'lognorm',\n",
    "#                               \"beta\",\n",
    "#                               \"burr\",\n",
    "#                               \"norm\"])\n",
    "#     f.fit()\n",
    "#     f.summary()\n",
    "\n",
    "#     sns.set_style('white')\n",
    "#     sns.set_context(\"paper\", font_scale = 2)\n",
    "#     sns.displot(data=dataset, x=\"Height\", kind=\"hist\", bins = 100, aspect = 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93def9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 clients---> gender, race, income\n",
    "pdfs = []\n",
    "for i in list(client_loss.keys()):\n",
    "    if(i== len(list(client_loss.keys()))-1):\n",
    "        continue\n",
    "    print(\"client_\",i, \" loss: \", client_loss[i],\"freq: \", client_frequency[i])\n",
    "\n",
    "#     shape, loc, scale = stats.gamma.fit(client_loss[i])\n",
    "    \n",
    "#     # Printing the estimated parameters\n",
    "#     print(\"Shape:\", shape)\n",
    "#     print(\"Location:\", loc)\n",
    "#     print(\"Scale:\", scale)\n",
    "#     pdf = stats.gamma.pdf(client_loss[i], shape, loc=loc, scale=scale)\n",
    "    \n",
    "    \n",
    "    df_mean = np.mean(client_loss[i])\n",
    "    df_std = np.std(client_loss[i])\n",
    "    pdf = stats.norm.pdf(client_loss[i], df_mean, df_std)\n",
    "    pdfs.append(pdf)\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    ax = sns.displot( x = pdf, kind = \"kde\", height=10, aspect=2,\n",
    "                linewidth = 5 )\n",
    "    ax.fig.suptitle('Original distribution', size = 20)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xlabel(\"Loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c9992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pdfs)):\n",
    "    for j in range(i+1,len(pdfs)):\n",
    "        print(i,\" and \", j , stats.wasserstein_distance(pdfs[i], pdfs[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bbbe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape, loc, scale = stats.gamma.fit(global_loss_list)\n",
    "pdf_global = stats.gamma.pdf(global_loss_list, shape, loc=loc, scale=scale)\n",
    "\n",
    "for i in range(len(pdfs)):\n",
    "    print(i,\" and global: \" , stats.wasserstein_distance(pdfs[i], pdf_global))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d36ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment plotter\n",
    "\n",
    "# for i in list(client_loss.keys()):\n",
    "#     if(i== len(list(client_loss.keys()))-1):\n",
    "#         continue\n",
    "#     print(\"client_\",i, \" loss: \", client_loss[i],\"freq: \", client_frequency[i])\n",
    "    \n",
    "# #     df_mean = np.mean(client_loss[i])\n",
    "# #     df_std = np.std(client_loss[i])\n",
    "# #     pdf = stats.norm.pdf(client_loss[i], df_mean, df_std)\n",
    "\n",
    "#     shape, loc, scale = stats.gamma.fit(client_loss[i])\n",
    "    \n",
    "#     # Printing the estimated parameters\n",
    "# #     print(\"Shape:\", shape)\n",
    "# #     print(\"Location:\", loc)\n",
    "# #     print(\"Scale:\", scale)\n",
    "#     pdf = stats.gamma.pdf(client_loss[i], shape, loc=loc, scale=scale)\n",
    "\n",
    "# #     plt.plot(client_loss[i], pdf, \"-o\", label = i)\n",
    "\n",
    "# #     client_loss[i] = stats.gamma.rvs(1, size=5000)+5\n",
    "#     sns.set_style(\"whitegrid\")\n",
    "\n",
    "#     ax = sns.displot( x = pdf, kind = \"kde\", height=10, aspect=2,\n",
    "#                 linewidth = 5 )\n",
    "#     ax.fig.suptitle('Original distribution', size = 20)\n",
    "# #     plt.plot(client_loss[i], client_frequency[i], \"-o\", label = i)\n",
    "# #     plt.legend()\n",
    "#     plt.ylabel(\"Frequency\")\n",
    "#     plt.xlabel(\"Loss\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670d1d42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T00:03:08.033027Z",
     "start_time": "2021-07-16T00:03:07.867893Z"
    }
   },
   "outputs": [],
   "source": [
    "score = global_model.evaluate(X_test, y_test, verbose=0)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcaa73e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T00:03:08.653744Z",
     "start_time": "2021-07-16T00:03:08.484057Z"
    }
   },
   "outputs": [],
   "source": [
    "nn_preds = global_model.predict(X_test)\n",
    "nn_preds = (nn_preds > 0.5)\n",
    "\n",
    "nn_precision =precision_score(y_test, nn_preds)\n",
    "nn_recall = recall_score(y_test, nn_preds)\n",
    "nn_accuracy = accuracy_score(y_test, nn_preds)\n",
    "nn_f1 = f1_score(y_test, nn_preds)\n",
    "\n",
    "\n",
    "print(\"Precision = {}\".format(nn_precision))\n",
    "print(\"Recall = {}\".format(nn_recall))\n",
    "print(\"Accuracy = {}\".format(nn_accuracy))\n",
    "print(\"f1 = {}\".format(nn_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6360dd5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T00:03:09.275052Z",
     "start_time": "2021-07-16T00:03:09.268152Z"
    }
   },
   "outputs": [],
   "source": [
    "arr = nn_preds > 0.5\n",
    "\n",
    "unique, counts = np.unique(arr, return_counts=True)\n",
    "\n",
    "np.asarray((unique, counts)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a5ee99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-14T19:56:25.650336Z",
     "start_time": "2021-07-14T19:56:25.620Z"
    }
   },
   "outputs": [],
   "source": [
    "# pdf for client losses\n",
    "# x-loss\n",
    "# y- frequency\n",
    "# each client has one pdf for all rounds\n",
    "# using histogram\n",
    "\n",
    "\n",
    "# drop client\n",
    "# non iid\n",
    "# fedavg\n",
    "\n",
    "\n",
    "# gender dist\n",
    "# kernel density\n",
    "\n",
    "\n",
    "# non-iid, loss to accuracy\n",
    "# 4client"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
