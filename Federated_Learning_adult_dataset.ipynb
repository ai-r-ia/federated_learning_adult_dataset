{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cb54a96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T12:48:49.512456Z",
     "start_time": "2021-07-15T12:48:46.946937Z"
    }
   },
   "source": [
    "# Federated Learning Implementation with tensorflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b29fd2ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T23:57:57.329013Z",
     "start_time": "2021-07-15T23:57:57.321303Z"
    }
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Adult Dataset Salary Prediction \n",
    "# This is part of a study to investigate Differetinal privacy in Machine learning, Naturally we wish to compare it with federated learning.\n",
    "\n",
    "\n",
    "\n",
    "# Refrences:\n",
    "\n",
    "# [1] Federated Learning with Non-IID Data, Yue Zhao et al, arXiv: 1806.00582v1, 2 Jun 2018\n",
    "# [2] Communication-Efficient Learning of Deep Networks from Decentralized Data, H. Brendan McMahan et al, arXiv:1602.05629v3 [cs.LG] 28 Feb 2017\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09b7dd46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T23:57:57.590963Z",
     "start_time": "2021-07-15T23:57:57.572204Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\riash\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\riash\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Users\\riash\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import precision_score,recall_score, accuracy_score,confusion_matrix,f1_score\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00b9bf6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T23:59:52.574848Z",
     "start_time": "2021-07-15T23:59:52.208912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9768, 94), (9768, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Train/Test Data seperation\n",
    "\n",
    "file_out = pd.read_csv('Data/adult_processed.csv')\n",
    "cols = []\n",
    "for i in list(file_out.columns):\n",
    "    if  i != 'income':\n",
    "        cols.append(i)\n",
    "\n",
    "feature_set1 = pd.read_csv('Data/train.csv')\n",
    "feature_set2 = pd.read_csv('Data/test.csv')\n",
    "\n",
    "x = feature_set1[cols].copy().values\n",
    "y = feature_set1[['income']].copy().values\n",
    "        \n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(x)\n",
    "y_train = y\n",
    "\n",
    "x2 = feature_set2[cols].copy().values\n",
    "y2 = feature_set2[['income']].copy().values\n",
    "        \n",
    "X_test = sc.transform(x2)\n",
    "y_test = y2\n",
    "\n",
    "\n",
    "X_test.shape, y_test.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "066207c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T23:59:51.783759Z",
     "start_time": "2021-07-15T23:59:51.727576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])\n",
      "0 1435\n",
      "1 3280\n",
      "2 5043\n",
      "3 2378\n",
      "4 246\n",
      "5 2870\n",
      "6 3198\n",
      "7 328\n",
      "8 3034\n",
      "9 2952\n",
      "10 1271\n",
      "11 4633\n",
      "12 2173\n",
      "13 4387\n",
      "14 1845\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_clients(image_list, label_list, num_clients=10, initial='clients'):\n",
    "    \n",
    "    ''' return: a dictionary with keys clients' names and value as \n",
    "                data shards - tuple of images and label lists.\n",
    "        args: \n",
    "            image_list: a list of numpy arrays of training images\n",
    "            label_list:a list of binarized labels for each image\n",
    "            num_client: number of fedrated members (clients)\n",
    "            initials: the clients'name prefix, e.g, clients_1 \n",
    "            \n",
    "    '''\n",
    "\n",
    "    #create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
    "\n",
    "#     iid\n",
    "#     #randomize the data\n",
    "#     data = list(zip(image_list, label_list))\n",
    "#     random.shuffle(data)\n",
    "    \n",
    "   \n",
    "    \n",
    "#     non-iid //////////////////////////////////////\n",
    "#     max_y = np.argmax(label_list, axis=-1)\n",
    "#     sorted_zip = sorted(zip(max_y, label_list, image_list), key=lambda x: x[0])\n",
    "#     data = [(x,y) for _,y,x in sorted_zip]\n",
    "\n",
    "# LOGIC:\n",
    "#    image_list length-->total data items\n",
    "#    taking 50 images in each shard--> num_shards = image_list/41 gives 953 shards\n",
    "#    num_shard = num_shards + 1 if(image_list%50 >0) \n",
    "\n",
    "    num_shards, num_imgs = 953, 41\n",
    "    idx_shard = [i for i in range(num_shards)]\n",
    "    dict_users = {i: np.array([]) for i in range(num_clients)}\n",
    "    idxs = np.arange(num_shards*num_imgs)\n",
    "#     labels = dataset.train_labels.numpy()\n",
    "\n",
    "    label_list = np.argmax(label_list, axis=-1)\n",
    "#     print(idxs.shape, label_list.shape)\n",
    "    \n",
    "    idxs_labels = np.vstack((idxs, label_list))\n",
    "    idxs_labels = idxs_labels[:, idxs_labels[1, :].argsort()]\n",
    "    idxs = idxs_labels[0, :]\n",
    "\n",
    "    min_shard = 1\n",
    "    max_shard = 60  #953/15 = 63.53\n",
    "    \n",
    "    random_shard_size = np.random.randint(min_shard, max_shard+1,\n",
    "                                          size=num_clients)\n",
    "    random_shard_size = np.around(random_shard_size /\n",
    "                                  sum(random_shard_size) * num_shards)\n",
    "    random_shard_size = random_shard_size.astype(int)\n",
    "\n",
    "\n",
    "    if sum(random_shard_size) > num_shards:\n",
    "\n",
    "        for i in range(num_clients):\n",
    "            # First assign each client 1 shard to ensure every client has\n",
    "            # atleast one shard of data\n",
    "            rand_set = set(np.random.choice(idx_shard, 1, replace=False))\n",
    "            idx_shard = list(set(idx_shard) - rand_set)\n",
    "            for rand in rand_set:\n",
    "                dict_users[i] = np.concatenate(\n",
    "                    (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]),\n",
    "                    axis=0)\n",
    "\n",
    "        random_shard_size = random_shard_size-1\n",
    "\n",
    "        # Next, randomly assign the remaining shards\n",
    "        for i in range(num_clients):\n",
    "            if len(idx_shard) == 0:\n",
    "                continue\n",
    "            shard_size = random_shard_size[i]\n",
    "            if shard_size > len(idx_shard):\n",
    "                shard_size = len(idx_shard)\n",
    "            rand_set = set(np.random.choice(idx_shard, shard_size,\n",
    "                                            replace=False))\n",
    "            idx_shard = list(set(idx_shard) - rand_set)\n",
    "            for rand in rand_set:\n",
    "                dict_users[i] = np.concatenate(\n",
    "                    (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]),\n",
    "                    axis=0)\n",
    "    else:\n",
    "\n",
    "        for i in range(num_clients):\n",
    "            shard_size = random_shard_size[i]\n",
    "            rand_set = set(np.random.choice(idx_shard, shard_size,\n",
    "                                            replace=False))\n",
    "            idx_shard = list(set(idx_shard) - rand_set)\n",
    "            for rand in rand_set:\n",
    "                dict_users[i] = np.concatenate(\n",
    "                    (dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]),\n",
    "                    axis=0)\n",
    "\n",
    "        if len(idx_shard) > 0:\n",
    "            # Add the leftover shards to the client with minimum images:\n",
    "            shard_size = len(idx_shard)\n",
    "            # Add the remaining shard to the client with lowest data\n",
    "            k = min(dict_users, key=lambda x: len(dict_users.get(x)))\n",
    "            rand_set = set(np.random.choice(idx_shard, shard_size,\n",
    "                                            replace=False))\n",
    "            idx_shard = list(set(idx_shard) - rand_set)\n",
    "            for rand in rand_set:\n",
    "                dict_users[k] = np.concatenate(\n",
    "                    (dict_users[k], idxs[rand*num_imgs:(rand+1)*num_imgs]),\n",
    "                    axis=0)\n",
    "#     print(dict_users[0])\n",
    "                \n",
    "    return dict_users\n",
    "\n",
    "#   ////////////////////////////////////////////////////  \n",
    "    #shard data and place at each client\n",
    "#     size = len(data)//num_clients\n",
    "#     shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "\n",
    "#     #number of clients must equal number of shards\n",
    "#     assert(len(shards) == len(client_names))\n",
    "\n",
    "#     print(len(image_list))\n",
    "#     for i in range(len(client_names)):\n",
    "#         print(client_names[i], len(shards[i]))\n",
    "              \n",
    "#     return {client_names[i] : shards[i] for i in range(len(client_names))} \n",
    "\n",
    "\n",
    "clients = create_clients(X_train, y_train, num_clients=15, initial='client')\n",
    "print(clients.keys())\n",
    "\n",
    "for i in range(len(clients.keys())):\n",
    "        print(i, len(clients[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "715ed657",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T23:59:52.579114Z",
     "start_time": "2021-07-15T23:59:52.576244Z"
    }
   },
   "outputs": [],
   "source": [
    "# take bs = 128 for 5 clients and 10 rounds\n",
    "def batch_data(data_shard, bs=64):\n",
    "    '''Takes in a clients data shard and create a tfds object off it\n",
    "    args:\n",
    "        shard: a data, label constituting a client's data shard\n",
    "        bs:batch size\n",
    "    return:\n",
    "        tfds object'''\n",
    "    #seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    \n",
    "    return dataset.shuffle(len(label)).batch(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfd80728",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T23:59:54.563066Z",
     "start_time": "2021-07-15T23:59:52.773340Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 94), (None, 1)), types: (tf.float64, tf.int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#process and batch the training data for each client\n",
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)\n",
    "    \n",
    "#process and batch the test set  \n",
    "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))\n",
    "test_batched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9dcf2ff7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-15T23:59:54.568193Z",
     "start_time": "2021-07-15T23:59:54.564443Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleMLP:\n",
    "    @staticmethod\n",
    "    def build(shape, classes=2 , learning_rate = 0.001, metric = \"accuracy\"):\n",
    "\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Dense(128, input_shape = (shape,)))\n",
    "#         model.add(Dense(128, Activation(\"relu\")))\n",
    "#         model.add(Dense(64, Activation(\"relu\")))\n",
    "#         model.add(Dense(32, Activation(\"relu\")))\n",
    "#         model.add(Dense(1))\n",
    "        \n",
    "        model.add(Dense(128, Activation(\"tanh\")))\n",
    "        model.add(Dense(64, Activation(\"tanh\")))\n",
    "        model.add(Dense(32, Activation(\"tanh\")))\n",
    "        model.add(Dense(1,Activation('sigmoid')))\n",
    "        \n",
    "\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f71dc14a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T00:00:30.156165Z",
     "start_time": "2021-07-16T00:00:30.152576Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = 0.005\n",
    "comms_round = 3\n",
    "loss=tf.keras.losses.BinaryCrossentropy(from_logits = False)\n",
    "\n",
    "metrics = ['binary_accuracy']\n",
    "\n",
    "optimizer = SGD(learning_rate=lr, \n",
    "                decay=lr / comms_round, \n",
    "                momentum=0.5\n",
    "               )     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eebf9376",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T00:00:30.412962Z",
     "start_time": "2021-07-16T00:00:30.399142Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def weight_scalling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    #get the bs\n",
    "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    #first calculate the total training data points across clinets\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
    "    # get the total number of data points held by a client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
    "    return local_count/global_count\n",
    "\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    '''function for scaling a models weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final\n",
    "\n",
    "\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "#     print(len(scaled_weight_list))\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "#         print(len(grad_list_tuple))\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "\n",
    "\n",
    "def test_model(X_test, Y_test,  model, comm_round):\n",
    "    \n",
    "#     cce = tf.keras.losses.BinaryCrossentropy(from_logits = True)\n",
    "#     cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    logits = model.predict(X_test)\n",
    "\n",
    "    score = global_model.evaluate(X_test, y_test, verbose=0)\n",
    "    acc = score[1] ; loss = score[0]\n",
    "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
    "\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4aef41d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T00:01:47.756094Z",
     "start_time": "2021-07-16T00:00:30.586083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 2ms/step - loss: 0.8523 - binary_accuracy: 0.7819\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.8912 - binary_accuracy: 0.7823\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.8019 - binary_accuracy: 0.7880\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 8.7930 - binary_accuracy: 0.3802\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 3.0038 - binary_accuracy: 0.7423\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 1.1724 - binary_accuracy: 0.7657\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.9001 - binary_accuracy: 0.7992\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 6.4385 - binary_accuracy: 0.5227\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 1.0163 - binary_accuracy: 0.7734\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 1.0699 - binary_accuracy: 0.7485\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 1.0213 - binary_accuracy: 0.7485\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 1.4702 - binary_accuracy: 0.7604\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 0.9014 - binary_accuracy: 0.7654\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.9820 - binary_accuracy: 0.7654\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 1.0498 - binary_accuracy: 0.7519\n",
      "comm_round: 0 | global_acc: 24.120% | global_loss: 11.571194648742676\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 11.4135 - binary_accuracy: 0.2515\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 11.6185 - binary_accuracy: 0.2381\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 11.4311 - binary_accuracy: 0.2504\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 11.7239 - binary_accuracy: 0.2312\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 11.5072 - binary_accuracy: 0.2454\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 11.4955 - binary_accuracy: 0.2462\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 11.4838 - binary_accuracy: 0.2469\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 11.6712 - binary_accuracy: 0.2346\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 11.4428 - binary_accuracy: 0.2496\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 11.9815 - binary_accuracy: 0.2143\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 11.5833 - binary_accuracy: 0.2404\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 11.6478 - binary_accuracy: 0.2362\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 11.6243 - binary_accuracy: 0.2377\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 11.6009 - binary_accuracy: 0.2392\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 11.8996 - binary_accuracy: 0.2197\n",
      "comm_round: 1 | global_acc: 24.120% | global_loss: 11.571194648742676\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 11.6243 - binary_accuracy: 0.2377\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 11.5072 - binary_accuracy: 0.2454\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 11.4311 - binary_accuracy: 0.2504\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 11.4838 - binary_accuracy: 0.2469\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 11.4955 - binary_accuracy: 0.2462\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 11.4135 - binary_accuracy: 0.2515\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 11.6477 - binary_accuracy: 0.2362\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 11.6009 - binary_accuracy: 0.2392\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 11.5833 - binary_accuracy: 0.2404\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 11.7239 - binary_accuracy: 0.2312\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 11.9815 - binary_accuracy: 0.2143\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 11.6712 - binary_accuracy: 0.2346\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 11.8996 - binary_accuracy: 0.2197\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 11.4428 - binary_accuracy: 0.2496\n",
      "41/41 [==============================] - 0s 2ms/step - loss: 11.6185 - binary_accuracy: 0.2381\n",
      "comm_round: 2 | global_acc: 24.120% | global_loss: 11.571194648742676\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#initialize global model\n",
    "smlp_global = SimpleMLP()\n",
    "global_model = smlp_global.build(X_train.shape[1] ,classes=2)\n",
    "global_model.compile(optimizer=optimizer, loss=loss, metrics=metrics) \n",
    "\n",
    "        \n",
    "#commence global training loop\n",
    "for comm_round in range(comms_round):\n",
    "            \n",
    "    # get the global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "    \n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    #randomize client data - using keys\n",
    "    client_names= list(clients_batched.keys())\n",
    "    random.shuffle(client_names)\n",
    "    \n",
    "    #loop through each client and create new local model\n",
    "    for client in client_names:\n",
    "        smlp_local = SimpleMLP()\n",
    "        local_model = smlp_local.build(X_train.shape[1],classes=2)\n",
    "        local_model.compile(loss=loss, \n",
    "                      optimizer=optimizer, \n",
    "                      metrics=metrics)\n",
    "        \n",
    "        #set local model weight to the weight of the global model\n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        #fit local model with client's data\n",
    "        local_model.fit(clients_batched[client], epochs=1, verbose=1)\n",
    "        \n",
    "#         get client acc, loss\n",
    "#         client_score = local_model.evaluate()\n",
    "#         print(metrics)\n",
    "        \n",
    "        #scale the model weights and add to list\n",
    "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "        \n",
    "        #clear session to free memory after each communication round\n",
    "        K.clear_session()\n",
    "        \n",
    "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    \n",
    "    #update global model \n",
    "    global_model.set_weights(average_weights)\n",
    "\n",
    "#     test global model and print out metrics after each communications round\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "670d1d42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T00:03:08.033027Z",
     "start_time": "2021-07-16T00:03:07.867893Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.49887731671333313, 0.7711916565895081]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = global_model.evaluate(X_test, y_test, verbose=0)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fcaa73e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T00:03:08.653744Z",
     "start_time": "2021-07-16T00:03:08.484057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.5224323322209863\n",
      "Recall = 0.5980475382003395\n",
      "Accuracy = 0.7711916461916462\n",
      "f1 = 0.5576885018800712\n"
     ]
    }
   ],
   "source": [
    "nn_preds = global_model.predict(X_test)\n",
    "nn_preds = (nn_preds > 0.5)\n",
    "\n",
    "nn_precision =precision_score(y_test, nn_preds)\n",
    "nn_recall = recall_score(y_test, nn_preds)\n",
    "nn_accuracy = accuracy_score(y_test, nn_preds)\n",
    "nn_f1 = f1_score(y_test, nn_preds)\n",
    "\n",
    "\n",
    "print(\"Precision = {}\".format(nn_precision))\n",
    "print(\"Recall = {}\".format(nn_recall))\n",
    "print(\"Accuracy = {}\".format(nn_accuracy))\n",
    "print(\"f1 = {}\".format(nn_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6360dd5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T00:03:09.275052Z",
     "start_time": "2021-07-16T00:03:09.268152Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0, 7071],\n",
       "       [   1, 2697]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = nn_preds > 0.5\n",
    "\n",
    "unique, counts = np.unique(arr, return_counts=True)\n",
    "\n",
    "np.asarray((unique, counts)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12a5ee99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-14T19:56:25.650336Z",
     "start_time": "2021-07-14T19:56:25.620Z"
    }
   },
   "outputs": [],
   "source": [
    "# pdf for client losses\n",
    "# x-loss\n",
    "# y- frequency\n",
    "# each client has one pdf for all rounds\n",
    "# using histogram\n",
    "\n",
    "\n",
    "# drop client\n",
    "# non iid\n",
    "# fedavg\n",
    "\n",
    "\n",
    "# gender dist"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
